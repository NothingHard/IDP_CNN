{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# VGG_MEAN = [123.68, 116.779, 103.939] # [R, G, B]\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] # [R, G, B]\n",
    "class VGG16:\n",
    "    def __init__(self, vgg16_npy_path=None):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        \n",
    "        # load pre-trained weights\n",
    "        if vgg16_npy_path is None:\n",
    "            path = inspect.getfile(Vgg16)\n",
    "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "            vgg16_npy_path = os.path.join(path, \"vgg16.npy\")\n",
    "            print(vgg16_npy_path)     \n",
    "        self.data_dict = np.load(vgg16_npy_path,encoding='latin1').item()\n",
    "        print(\"npy file loaded\")\n",
    "        \n",
    "        # input information\n",
    "        self.H, self.W, self.C = 32, 32, 3\n",
    "        self.classes = 10\n",
    "        \n",
    "        # operation dictionary\n",
    "        self.prob_dict = {}\n",
    "        self.loss_dict = {}\n",
    "        self.accu_dict = {}\n",
    "        \n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.classes])\n",
    "\n",
    "    def build(self, dp, prof_type):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dp = dp \n",
    "        print(\"Will optimize at DP=\", self.dp)\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        rgb_scaled = self.x * 255.0\n",
    "\n",
    "        # normalize input by VGG_MEAN\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert   red.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert  blue.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        self.x = tf.concat(axis=3, values=[\n",
    "              blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "             red - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "        # declare and initialize the weights of VGG16\n",
    "        with tf.variable_scope(\"VGG16\"):\n",
    "        \n",
    "            self.conv1_1_W, self.conv1_1_b = self.get_conv_filter(\"conv1_1\"), self.get_bias(\"conv1_1\")\n",
    "            self.conv1_2_W, self.conv1_2_b = self.get_conv_filter(\"conv1_2\"), self.get_bias(\"conv1_2\")\n",
    "\n",
    "            self.conv2_1_W, self.conv2_1_b = self.get_conv_filter(\"conv2_1\"), self.get_bias(\"conv2_1\")\n",
    "            self.conv2_2_W, self.conv2_2_b = self.get_conv_filter(\"conv2_2\"), self.get_bias(\"conv2_2\")\n",
    "\n",
    "            self.conv3_1_W, self.conv3_1_b = self.get_conv_filter(\"conv3_1\"), self.get_bias(\"conv3_1\")\n",
    "            self.conv3_2_W, self.conv3_2_b = self.get_conv_filter(\"conv3_2\"), self.get_bias(\"conv3_2\")\n",
    "            self.conv3_3_W, self.conv3_3_b = self.get_conv_filter(\"conv3_3\"), self.get_bias(\"conv3_3\")\n",
    "\n",
    "            self.conv4_1_W, self.conv4_1_b = self.get_conv_filter(\"conv4_1\"), self.get_bias(\"conv4_1\")\n",
    "            self.conv4_2_W, self.conv4_2_b = self.get_conv_filter(\"conv4_2\"), self.get_bias(\"conv4_2\")\n",
    "            self.conv4_3_W, self.conv4_3_b = self.get_conv_filter(\"conv4_3\"), self.get_bias(\"conv4_3\")\n",
    "\n",
    "            self.conv5_1_W, self.conv5_1_b = self.get_conv_filter(\"conv5_1\"), self.get_bias(\"conv5_1\")\n",
    "            self.conv5_2_W, self.conv5_2_b = self.get_conv_filter(\"conv5_2\"), self.get_bias(\"conv5_2\")\n",
    "            self.conv5_3_W, self.conv5_3_b = self.get_conv_filter(\"conv5_3\"), self.get_bias(\"conv5_3\")\n",
    "\n",
    "            self.fc_1_W = tf.get_variable(name=\"fc_1_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_1_b = tf.get_variable(name=\"fc_1_b\", shape=(512), initializer=tf.ones_initializer(), dtype=tf.float32)\n",
    "\n",
    "            self.fc_2_W = tf.get_variable(name=\"fc_2_W\", shape=(512, 10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_2_b = tf.get_variable(name=\"fc_2_b\", shape=(10), initializer=tf.ones_initializer(), dtype=tf.float32)\n",
    "        \n",
    "        # create operations at every dot product percentages\n",
    "        for dp_i in dp:\n",
    "            with tf.name_scope(str(int(dp_i*100))):\n",
    "                conv1_1 = self.idp_conv_layer( self.x, \"conv1_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv1_2 = self.idp_conv_layer(conv1_1, \"conv1_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "                conv2_1 = self.idp_conv_layer(  pool1, \"conv2_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv2_2 = self.idp_conv_layer(conv2_1, \"conv2_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "                conv3_1 = self.idp_conv_layer(  pool2, \"conv3_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_2 = self.idp_conv_layer(conv3_1, \"conv3_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_3 = self.idp_conv_layer(conv3_2, \"conv3_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "                conv4_1 = self.idp_conv_layer(  pool3, \"conv4_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_2 = self.idp_conv_layer(conv4_1, \"conv4_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_3 = self.idp_conv_layer(conv4_2, \"conv4_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool4 = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "                conv5_1 = self.idp_conv_layer(  pool4, \"conv5_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_2 = self.idp_conv_layer(conv5_1, \"conv5_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_3 = self.idp_conv_layer(conv5_2, \"conv5_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "                fc_1 = self.fc_layer(pool5, 'fc_1')\n",
    "                fc_1 = tf.nn.dropout(fc_1, keep_prob=0.5)\n",
    "                fc_1 = tf.nn.relu(fc_1)\n",
    "                \n",
    "                logits = tf.nn.bias_add(tf.matmul( fc_1, self.fc_2_W), self.fc_2_b)\n",
    "                prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "                \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "                loss = tf.reduce_mean(cross_entropy)\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(x=tf.argmax(logits, 1), y=tf.argmax(self.y, 1)),tf.float32))\n",
    "                \n",
    "                self.prob_dict[str(int(dp_i*100))] = prob\n",
    "                self.loss_dict[str(int(dp_i*100))] = loss\n",
    "                self.accu_dict[str(int(dp_i*100))] = accuracy\n",
    "                \n",
    "                tf.summary.scalar(name=\"accu_at_\"+str(int(dp_i*100)), tensor=accuracy)\n",
    "                tf.summary.scalar(name=\"loss_at_\"+str(int(dp_i*100)), tensor=loss)\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def idp_conv_layer(self, bottom, name, dp, prof_type, gamma_trainable = False):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "            \n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "        \n",
    "            # get profile\n",
    "            profile = self.get_profile(O, prof_type)\n",
    "            \n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32'))\n",
    "            if len(profile) == len(mask):\n",
    "                profile *= mask\n",
    "            else:\n",
    "                raise ValueError(\"profile and mask must have the same shape.\")\n",
    "\n",
    "            # create a profile coefficient, gamma\n",
    "            filter_profile = np.stack([profile for i in range(H*W*C)])\n",
    "            filter_profile = np.reshape(filter_profile, newshape=(H, W, C, O))\n",
    "            \n",
    "            # gamma in use\n",
    "            gamma_W = tf.Variable(initial_value=filter_profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "            gamma_b = tf.Variable(initial_value=profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "\n",
    "            # IDP conv2d output\n",
    "            conv_filter = tf.multiply(conv_filter, gamma_W)\n",
    "            conv_biases = tf.multiply(conv_biases, gamma_b)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "            relu = tf.nn.relu(conv)\n",
    "            \n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.name_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            \n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                weights = tf.get_variable(name=name+\"_W\")\n",
    "                biases = tf.get_variable(name=name+\"_b\")\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "    def get_bias(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][1], name=name+\"_b\")\n",
    "#     def get_fc_weight(self, name):\n",
    "#         return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "\n",
    "    def get_profile(self, C, prof_type):\n",
    "        def half_exp(n, k=1, dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - n_ones\n",
    "            return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "        if prof_type == \"linear\":\n",
    "            profile = np.linspace(1.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "        elif prof_type == \"all-one\":\n",
    "            profile = np.ones(C, dtype='float32')\n",
    "        elif prof_type == \"half-exp\":\n",
    "            profile = half_exp(C, 2.0)\n",
    "        elif prof_type == \"harmonic\":\n",
    "            profile = np.array(1.0/(np.arange(C)+1))\n",
    "        else:\n",
    "            raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "        return profile\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "VGG_MEAN = [123.68, 116.779, 103.939] # [R, G, B]\n",
    "\n",
    "class VGG16:\n",
    "    def __init__(self, vgg16_npy_path=None):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        \n",
    "        # load pre-trained weights\n",
    "        if vgg16_npy_path is None:\n",
    "            path = inspect.getfile(Vgg16)\n",
    "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "            path = os.path.join(path, \"vgg16_weights.npz\")\n",
    "            vgg16_npy_path = path\n",
    "            print(path)     \n",
    "        self.data_dict = np.load(vgg16_npy_path)\n",
    "        print(\"npy file loaded\")\n",
    "        \n",
    "        # input information\n",
    "        self.H, self.W, self.C = 32, 32, 3\n",
    "        self.classes = 10\n",
    "        \n",
    "        # operation dictionary\n",
    "        self.prob_dict = {}\n",
    "        self.loss_dict = {}\n",
    "        self.accu_dict = {}\n",
    "        \n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.classes])\n",
    "\n",
    "    def build(self, dp, prof_type):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dp = dp \n",
    "        print(\"Will optimize at DP=\", self.dp)\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        self.x = self.x * 255.0\n",
    "\n",
    "        # normalize input by VGG_MEAN\n",
    "#         red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "#         assert   red.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "#         assert green.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "#         assert  blue.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "#         self.x = tf.concat(axis=3, values=[\n",
    "#               blue - VGG_MEAN[0],\n",
    "#             green - VGG_MEAN[1],\n",
    "#              red - VGG_MEAN[2],\n",
    "#         ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1,1,1,3], name='img_mean')\n",
    "        self.x = self.x - mean\n",
    "        \n",
    "        # declare and initialize the weights of VGG16\n",
    "        with tf.variable_scope(\"vgg16\"):\n",
    "        \n",
    "            self.conv1_1_W, self.conv1_1_b = self.get_conv_filter(\"conv1_1\"), self.get_bias(\"conv1_1\")\n",
    "            self.conv1_2_W, self.conv1_2_b = self.get_conv_filter(\"conv1_2\"), self.get_bias(\"conv1_2\")\n",
    "\n",
    "            self.conv2_1_W, self.conv2_1_b = self.get_conv_filter(\"conv2_1\"), self.get_bias(\"conv2_1\")\n",
    "            self.conv2_2_W, self.conv2_2_b = self.get_conv_filter(\"conv2_2\"), self.get_bias(\"conv2_2\")\n",
    "\n",
    "            self.conv3_1_W, self.conv3_1_b = self.get_conv_filter(\"conv3_1\"), self.get_bias(\"conv3_1\")\n",
    "            self.conv3_2_W, self.conv3_2_b = self.get_conv_filter(\"conv3_2\"), self.get_bias(\"conv3_2\")\n",
    "            self.conv3_3_W, self.conv3_3_b = self.get_conv_filter(\"conv3_3\"), self.get_bias(\"conv3_3\")\n",
    "\n",
    "            self.conv4_1_W, self.conv4_1_b = self.get_conv_filter(\"conv4_1\"), self.get_bias(\"conv4_1\")\n",
    "            self.conv4_2_W, self.conv4_2_b = self.get_conv_filter(\"conv4_2\"), self.get_bias(\"conv4_2\")\n",
    "            self.conv4_3_W, self.conv4_3_b = self.get_conv_filter(\"conv4_3\"), self.get_bias(\"conv4_3\")\n",
    "\n",
    "            self.conv5_1_W, self.conv5_1_b = self.get_conv_filter(\"conv5_1\"), self.get_bias(\"conv5_1\")\n",
    "            self.conv5_2_W, self.conv5_2_b = self.get_conv_filter(\"conv5_2\"), self.get_bias(\"conv5_2\")\n",
    "            self.conv5_3_W, self.conv5_3_b = self.get_conv_filter(\"conv5_3\"), self.get_bias(\"conv5_3\")\n",
    "\n",
    "            self.fc_1_W = tf.get_variable(name=\"fc_1_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_1_b = tf.get_variable(name=\"fc_1_b\", shape=(512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "            self.fc_2_W = tf.get_variable(name=\"fc_2_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_2_b = tf.get_variable(name=\"fc_2_b\", shape=(512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "            self.fc_3_W = tf.get_variable(name=\"fc_3_W\", shape=(512, 10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_3_b = tf.get_variable(name=\"fc_3_b\", shape=(10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        # create operations at every dot product percentages\n",
    "        for dp_i in dp:\n",
    "            with tf.name_scope(str(int(dp_i*100))):\n",
    "                conv1_1 = self.idp_conv_layer( self.x, \"conv1_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv1_2 = self.idp_conv_layer(conv1_1, \"conv1_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "                conv2_1 = self.idp_conv_layer(  pool1, \"conv2_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv2_2 = self.idp_conv_layer(conv2_1, \"conv2_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "                conv3_1 = self.idp_conv_layer(  pool2, \"conv3_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_2 = self.idp_conv_layer(conv3_1, \"conv3_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_3 = self.idp_conv_layer(conv3_2, \"conv3_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "                conv4_1 = self.idp_conv_layer(  pool3, \"conv4_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_2 = self.idp_conv_layer(conv4_1, \"conv4_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_3 = self.idp_conv_layer(conv4_2, \"conv4_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool4 = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "                conv5_1 = self.idp_conv_layer(  pool4, \"conv5_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_2 = self.idp_conv_layer(conv5_1, \"conv5_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_3 = self.idp_conv_layer(conv5_2, \"conv5_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "#                 fc_1 = self.fc_layer(pool5, 'fc_1')\n",
    "#                 fc_1 = tf.nn.dropout(fc_1, keep_prob=0.5)\n",
    "#                 fc_1 = tf.nn.relu(fc_1)\n",
    "#                 fc_2 = self.fc_layer(fc_1, 'fc_2')\n",
    "                fc_2 = self.fc_layer(pool5, 'fc_2')\n",
    "                \n",
    "                fc_2 = tf.nn.dropout(fc_2, keep_prob=0.5)\n",
    "                fc_2 = tf.nn.relu(fc_2)\n",
    "                \n",
    "                logits = tf.nn.bias_add(tf.matmul( fc_2, self.fc_3_W), self.fc_3_b)\n",
    "                prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "                \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "                loss = tf.reduce_sum(cross_entropy)\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(x=tf.argmax(prob, 1), y=tf.argmax(self.y, 1)),tf.float32))\n",
    "                \n",
    "                self.prob_dict[str(int(dp_i*100))] = prob\n",
    "                self.loss_dict[str(int(dp_i*100))] = loss\n",
    "                self.accu_dict[str(int(dp_i*100))] = accuracy\n",
    "                \n",
    "                tf.summary.scalar(name=\"accu_at_\"+str(int(dp_i*100)), tensor=accuracy)\n",
    "                tf.summary.scalar(name=\"loss_at_\"+str(int(dp_i*100)), tensor=loss)\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def idp_conv_layer(self, bottom, name, dp, prof_type, gamma_trainable = False):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "            \n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "        \n",
    "            # get profile\n",
    "            profile = self.get_profile(O, prof_type)\n",
    "            \n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32'))\n",
    "            if len(profile) == len(mask):\n",
    "                profile *= mask\n",
    "            else:\n",
    "                raise ValueError(\"profile and mask must have the same shape.\")\n",
    "\n",
    "            # create a profile coefficient, gamma\n",
    "            filter_profile = np.stack([profile for i in range(H*W*C)])\n",
    "            filter_profile = np.reshape(filter_profile, newshape=(H, W, C, O))\n",
    "            \n",
    "            # gamma in use\n",
    "            gamma_W = tf.Variable(initial_value=filter_profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "            gamma_b = tf.Variable(initial_value=profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "\n",
    "            # IDP conv2d output\n",
    "            conv_filter = tf.multiply(conv_filter, gamma_W)\n",
    "            conv_biases = tf.multiply(conv_biases, gamma_b)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "            relu = tf.nn.relu(conv)\n",
    "            \n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.name_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            \n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                weights = tf.get_variable(name=name+\"_W\")\n",
    "                biases = tf.get_variable(name=name+\"_b\")\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_W\"], name=name+\"_W\")\n",
    "    def get_bias(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_b\"], name=name+\"_b\")\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_W\"], name=name+\"_W\")\n",
    "\n",
    "    def get_profile(self, C, prof_type):\n",
    "        def half_exp(n, k=1, dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - n_ones\n",
    "            return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "        if prof_type == \"linear\":\n",
    "            profile = np.linspace(1.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "        elif prof_type == \"all-one\":\n",
    "            profile = np.ones(C, dtype='float32')\n",
    "        elif prof_type == \"half-exp\":\n",
    "            profile = half_exp(C, 2.0)\n",
    "        elif prof_type == \"harmonic\":\n",
    "            profile = np.array(1.0/(np.arange(C)+1))\n",
    "        else:\n",
    "            raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "        return profile\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Functions for downloading the CIFAR-10 data-set from the internet\n",
    "# and loading it into memory.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "# Usage:\n",
    "# 1) Set the variable data_path with the desired storage path.\n",
    "# 2) Call maybe_download_and_extract() to download the data-set\n",
    "#    if it is not already located in the given data_path.\n",
    "# 3) Call load_class_names() to get an array of the class-names.\n",
    "# 4) Call load_training_data() and load_test_data() to get\n",
    "#    the images, class-numbers and one-hot encoded class-labels\n",
    "#    for the training-set and test-set.\n",
    "# 5) Use the returned data in your own program.\n",
    "#\n",
    "# Format:\n",
    "# The images for the training- and test-sets are returned as 4-dim numpy\n",
    "# arrays each with the shape: [image_number, height, width, channel]\n",
    "# where the individual pixels are floats between 0.0 and 1.0.\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class CIFAR10(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        ########################################################################\n",
    "        # Directory where you want to download and save the data-set.\n",
    "        # Set this before you start calling any of the functions below.\n",
    "        self.data_path = \"/home/cmchang/IDP_CNN/data/\"\n",
    "\n",
    "        # URL for the data-set on the internet.\n",
    "        self.data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "        ########################################################################\n",
    "        # Various constants for the size of the images.\n",
    "        # Use these constants in your own program.\n",
    "\n",
    "        # Width and height of each image.\n",
    "        self.img_size = 32\n",
    "\n",
    "        # Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "        self.num_channels = 3\n",
    "\n",
    "        # Length of an image when flattened to a 1-dim array.\n",
    "        self.img_size_flat = self.img_size * self.img_size * self.num_channels\n",
    "\n",
    "        # Number of classes.\n",
    "        self.num_classes = 10\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "    def _one_hot_encoded(self, class_numbers, num_classes=None):\n",
    "        \"\"\"\n",
    "        Generate the One-Hot encoded class-labels from an array of integers.\n",
    "        For example, if class_number=2 and num_classes=4 then\n",
    "        the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
    "        :param class_numbers:\n",
    "            Array of integers with class-numbers.\n",
    "            Assume the integers are from zero to num_classes-1 inclusive.\n",
    "        :param num_classes:\n",
    "            Number of classes. If None then use max(class_numbers)+1.\n",
    "        :return:\n",
    "            2-dim array of shape: [len(class_numbers), num_classes]\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the number of classes if None is provided.\n",
    "        # Assumes the lowest class-number is zero.\n",
    "        if num_classes is None:\n",
    "            num_classes = np.max(class_numbers) + 1\n",
    "\n",
    "        return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "    ########################################################################\n",
    "    # Private functions for downloading, unpacking and loading data-files.\n",
    "\n",
    "    def _get_file_path(self, filename=\"\"):\n",
    "        \"\"\"\n",
    "        Return the full path of a data-file for the data-set.\n",
    "\n",
    "        If filename==\"\" then return the directory of the files.\n",
    "        \"\"\"\n",
    "\n",
    "        return os.path.join(self.data_path, \"cifar-10-batches-py/\", filename)\n",
    "\n",
    "\n",
    "    def _unpickle(self, filename):\n",
    "        \"\"\"\n",
    "        Unpickle the given file and return the data.\n",
    "\n",
    "        Note that the appropriate dir-name is prepended the filename.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create full path for the file.\n",
    "        file_path = self._get_file_path(filename)\n",
    "\n",
    "        print(\"Loading data: \" + file_path)\n",
    "\n",
    "        with open(file_path, mode='rb') as file:\n",
    "            # In Python 3.X it is important to set the encoding,\n",
    "            # otherwise an exception is raised here.\n",
    "            data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def _convert_images(self, raw):\n",
    "        \"\"\"\n",
    "        Convert images from the CIFAR-10 format and\n",
    "        return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "        where the pixels are floats between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the raw images from the data-files to floating-points.\n",
    "        raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "        # Reshape the array to 4-dimensions.\n",
    "        images = raw_float.reshape([-1, self.num_channels, self.img_size, self.img_size])\n",
    "\n",
    "        # Reorder the indices of the array.\n",
    "        images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "    def _load_data(self, filename):\n",
    "        \"\"\"\n",
    "        Load a pickled data-file from the CIFAR-10 data-set\n",
    "        and return the converted images (see above) and the class-number\n",
    "        for each image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the pickled data-file.\n",
    "        data = self._unpickle(filename)\n",
    "\n",
    "        # Get the raw images.\n",
    "        raw_images = data[b'data']\n",
    "\n",
    "        # Get the class-numbers for each image. Convert to numpy-array.\n",
    "        cls = np.array(data[b'labels'])\n",
    "\n",
    "        # Convert the images.\n",
    "        images = self._convert_images(raw_images)\n",
    "\n",
    "        return images, cls\n",
    "\n",
    "    ########################################################################\n",
    "    # Public functions that you may call to download the data-set from\n",
    "    # the internet and load the data into memory.\n",
    "\n",
    "    def load_class_names(self):\n",
    "        \"\"\"\n",
    "        Load the names for the classes in the CIFAR-10 data-set.\n",
    "\n",
    "        Returns a list with the names. Example: names[3] is the name\n",
    "        associated with class-number 3.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the class-names from the pickled file.\n",
    "        raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
    "\n",
    "        # Convert from binary strings.\n",
    "        names = [x.decode('utf-8') for x in raw]\n",
    "\n",
    "        return names\n",
    "\n",
    "\n",
    "    def load_training_data(self):\n",
    "        \"\"\"\n",
    "        Load all the training-data for the CIFAR-10 data-set.\n",
    "\n",
    "        The data-set is split into 5 data-files which are merged here.\n",
    "\n",
    "        Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "        \"\"\"\n",
    "        # Number of files for the training-set.\n",
    "        _num_files_train = 5\n",
    "\n",
    "        # Number of images for each batch-file in the training-set.\n",
    "        _images_per_file = 10000\n",
    "\n",
    "        # Total number of images in the training-set.\n",
    "        # This is used to pre-allocate arrays for efficiency.\n",
    "        _num_images_train = _num_files_train * _images_per_file\n",
    "        \n",
    "        \n",
    "        # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "        images = np.zeros(shape=[_num_images_train, self.img_size, self.img_size, self.num_channels], dtype=float)\n",
    "        cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
    "\n",
    "        # Begin-index for the current batch.\n",
    "        begin = 0\n",
    "\n",
    "        # For each data-file.\n",
    "        for i in range(_num_files_train):\n",
    "            # Load the images and class-numbers from the data-file.\n",
    "            images_batch, cls_batch = self._load_data(filename=\"data_batch_\" + str(i + 1))\n",
    "\n",
    "            # Number of images in this batch.\n",
    "            num_images = len(images_batch)\n",
    "\n",
    "            # End-index for the current batch.\n",
    "            end = begin + num_images\n",
    "\n",
    "            # Store the images into the array.\n",
    "            images[begin:end, :] = images_batch\n",
    "\n",
    "            # Store the class-numbers into the array.\n",
    "            cls[begin:end] = cls_batch\n",
    "\n",
    "            # The begin-index for the next batch is the current end-index.\n",
    "            begin = end\n",
    "\n",
    "        return images, self._one_hot_encoded(class_numbers=cls, num_classes=self.num_classes)\n",
    "\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"\n",
    "        Load all the test-data for the CIFAR-10 data-set.\n",
    "\n",
    "        Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images, cls = self._load_data(filename=\"test_batch\")\n",
    "\n",
    "        return images, self._one_hot_encoded(class_numbers=cls, num_classes=self.num_classes)\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    ########################################################################\n",
    "    #\n",
    "    # Functions for downloading and extracting data-files from the internet.\n",
    "    #\n",
    "    # Implemented in Python 3.5\n",
    "    #\n",
    "    ########################################################################\n",
    "    #\n",
    "    # This file is part of the TensorFlow Tutorials available at:\n",
    "    #\n",
    "    # https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "    #\n",
    "    # Published under the MIT License. See the file LICENSE for details.\n",
    "    #\n",
    "    # Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "    #\n",
    "    ########################################################################\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import zipfile\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "    def _print_download_progress(self, count, block_size, total_size):\n",
    "        \"\"\"\n",
    "        Function used for printing the download progress.\n",
    "        Used as a call-back function in maybe_download_and_extract().\n",
    "        \"\"\"\n",
    "\n",
    "        # Percentage completion.\n",
    "        pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "        # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "        msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "        # Print it.\n",
    "        sys.stdout.write(msg)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "    def maybe_download_and_extract(self, url, download_dir):\n",
    "        \"\"\"\n",
    "        Download and extract the data if it doesn't already exist.\n",
    "        Assumes the url is a tar-ball file.\n",
    "        :param url:\n",
    "            Internet URL for the tar-file to download.\n",
    "            Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        :param download_dir:\n",
    "            Directory where the downloaded file is saved.\n",
    "            Example: \"data/CIFAR-10/\"\n",
    "        :return:\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Filename for saving the file downloaded from the internet.\n",
    "        # Use the filename from the URL and add it to the download_dir.\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        # Check if the file already exists.\n",
    "        # If it exists then we assume it has also been extracted,\n",
    "        # otherwise we need to download and extract it now.\n",
    "        if not os.path.exists(file_path):\n",
    "            # Check if the download directory exists, otherwise create it.\n",
    "            if not os.path.exists(download_dir):\n",
    "                os.makedirs(download_dir)\n",
    "\n",
    "            # Download the file from the internet.\n",
    "            file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                      filename=file_path,\n",
    "                                                      reporthook=self._print_download_progress)\n",
    "\n",
    "            print()\n",
    "            print(\"Download finished. Extracting files.\")\n",
    "\n",
    "            if file_path.endswith(\".zip\"):\n",
    "                # Unpack the zip-file.\n",
    "                zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "            elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "                # Unpack the tar-ball.\n",
    "                tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "            print(\"Done.\")\n",
    "        else:\n",
    "            print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "\n",
    "    ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    \n",
    "    # print([str(i.name) for i in not_initialized_vars])\n",
    "    \n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_1\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_2\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_3\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_4\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_5\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/test_batch\n",
      "npy file loaded\n",
      "Will optimize at DP= [0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001, 1.0]\n",
      "build model started\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'conv1_1 is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-16d910e95ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vgg16_weights.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#vgg16 = VGG16(\"vgg16.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprof_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all-one\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-27df7f84dad0>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, dp, prof_type)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vgg16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_1_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_1_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_conv_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_2_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_2_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_conv_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-27df7f84dad0>\u001b[0m in \u001b[0;36mget_conv_filter\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_conv_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not a file in the archive\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv1_1 is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import VGG16\n",
    "#import CIFAR10\n",
    "\n",
    "dataset = CIFAR10()\n",
    "Xtrain, Ytrain = dataset.load_training_data()\n",
    "Xtest, Ytest = dataset.load_test_data()\n",
    "\n",
    "###\n",
    "dp = [(i+1)*0.05 for i in range(1,20)]\n",
    "vgg16 = VGG16(\"vgg16_weights.npz\")\n",
    "#vgg16 = VGG16(\"vgg16.npy\")\n",
    "vgg16.build(dp=dp, prof_type=\"all-one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 200\n",
    "alpha = 0.5\n",
    "early_stop_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = vgg16.loss_dict['100']\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(obj)\n",
    "train_op = tf.train.GradientDescentOptimizer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\"log/\",sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    val_loss = sess.run(vgg16.accu_dict['100'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "                                   vgg16.y: Ytest[:5000,:]})\n",
    "    print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "    # training an epoch\n",
    "    for i in range(5):\n",
    "        st = i*batch_size\n",
    "        ed = (i+1)*batch_size\n",
    "        _, summary = sess.run([train_op, vgg16.summary_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                        vgg16.y: Ytrain[st:ed,:]})\n",
    "        summary_writer.add_summary(summary, i)\n",
    "#         # print(summary)\n",
    "#     val_loss = sess.run(vgg16.accu_dict['100'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "#     val_loss = sess.run(vgg16.accu_dict['70'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "#     val_loss = sess.run(vgg16.accu_dict['30'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = sorted([int(dp_i*100) for dp_i in dp],reverse=True)\n",
    "tasks = [str(task) for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.random.permutation(Xtest.shape[0])[:(int(Xtest.shape[0]/2))]\n",
    "# Xtest, Ytest= Xtest[idx,:], Ytest[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24580fcb3c4b699773cb8347ca3ab5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452c60b83caa46d3823c0e2a79d453a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 78.7 sec >> obj loss: 0.8100, task at 100: 0.7217\n",
      "Epoch 2 (0), 78.28 sec >> obj loss: 0.6649, task at 100: 0.7732\n",
      "Epoch 3 (0), 78.65 sec >> obj loss: 0.5973, task at 100: 0.7961\n",
      "Epoch 4 (0), 78.59 sec >> obj loss: 0.5464, task at 100: 0.8155\n",
      "Epoch 5 (0), 78.08 sec >> obj loss: 0.5087, task at 100: 0.8330\n",
      "Epoch 6 (1), 78.47 sec >> obj loss: 0.5217, task at 100: 0.8277\n",
      "Epoch 7 (0), 78.58 sec >> obj loss: 0.4970, task at 100: 0.8417\n",
      "Epoch 8 (0), 78.3 sec >> obj loss: 0.4895, task at 100: 0.8428\n",
      "Epoch 9 (1), 78.46 sec >> obj loss: 0.5340, task at 100: 0.8340\n",
      "Epoch 10 (2), 78.45 sec >> obj loss: 0.5117, task at 100: 0.8435\n",
      "Epoch 11 (3), 78.35 sec >> obj loss: 0.5248, task at 100: 0.8476\n",
      "Epoch 12 (4), 78.47 sec >> obj loss: 0.5334, task at 100: 0.8493\n",
      "Epoch 13 (0), 112.75 sec >> obj loss: 0.7562, task at 70: 0.6721\n",
      "Epoch 14 (0), 112.76 sec >> obj loss: 0.6974, task at 70: 0.7357\n",
      "Epoch 15 (0), 112.69 sec >> obj loss: 0.6612, task at 70: 0.7669\n",
      "Epoch 16 (1), 112.72 sec >> obj loss: 0.6621, task at 70: 0.7766\n",
      "Epoch 17 (2), 112.6 sec >> obj loss: 0.6843, task at 70: 0.7930\n",
      "Epoch 18 (3), 112.61 sec >> obj loss: 0.6636, task at 70: 0.8023\n",
      "Epoch 19 (0), 112.6 sec >> obj loss: 0.6513, task at 70: 0.8095\n",
      "Epoch 20 (1), 112.66 sec >> obj loss: 0.6863, task at 70: 0.8124\n",
      "Epoch 21 (2), 112.83 sec >> obj loss: 0.6965, task at 70: 0.8228\n",
      "Epoch 22 (3), 112.71 sec >> obj loss: 0.6871, task at 70: 0.8238\n",
      "Epoch 23 (4), 112.62 sec >> obj loss: 0.7185, task at 70: 0.8184\n",
      "Epoch 24 (0), 145.7 sec >> obj loss: 1.3858, task at 40: 0.1982\n",
      "Epoch 25 (0), 145.32 sec >> obj loss: 1.3216, task at 40: 0.2665\n",
      "Epoch 26 (0), 145.55 sec >> obj loss: 1.2057, task at 40: 0.3960\n",
      "Epoch 27 (0), 145.64 sec >> obj loss: 1.1070, task at 40: 0.4784\n",
      "Epoch 28 (0), 146.03 sec >> obj loss: 1.0336, task at 40: 0.5436\n",
      "Epoch 29 (1), 145.81 sec >> obj loss: 1.0670, task at 40: 0.5236\n",
      "Epoch 30 (0), 145.85 sec >> obj loss: 0.9784, task at 40: 0.6087\n",
      "Epoch 31 (0), 145.72 sec >> obj loss: 0.9563, task at 40: 0.6321\n",
      "Epoch 32 (1), 145.89 sec >> obj loss: 0.9699, task at 40: 0.6424\n",
      "Epoch 33 (0), 146.02 sec >> obj loss: 0.9455, task at 40: 0.6803\n",
      "Epoch 34 (0), 145.78 sec >> obj loss: 0.9421, task at 40: 0.6862\n",
      "Epoch 35 (0), 145.99 sec >> obj loss: 0.9268, task at 40: 0.6939\n",
      "Epoch 36 (0), 145.86 sec >> obj loss: 0.9163, task at 40: 0.7286\n",
      "Epoch 37 (0), 145.7 sec >> obj loss: 0.9034, task at 40: 0.7301\n",
      "Epoch 38 (1), 145.97 sec >> obj loss: 0.9377, task at 40: 0.7302\n",
      "Epoch 39 (0), 145.81 sec >> obj loss: 0.8952, task at 40: 0.7529\n",
      "Epoch 40 (1), 146.16 sec >> obj loss: 0.9247, task at 40: 0.7563\n",
      "Epoch 41 (2), 145.93 sec >> obj loss: 0.9081, task at 40: 0.7644\n",
      "Epoch 42 (3), 145.83 sec >> obj loss: 0.9334, task at 40: 0.7596\n",
      "Epoch 43 (4), 145.95 sec >> obj loss: 0.9744, task at 40: 0.7592\n",
      "Epoch 44 (0), 179.72 sec >> obj loss: 2.1222, task at 10: 0.0992\n",
      "Epoch 45 (0), 179.02 sec >> obj loss: 1.9382, task at 10: 0.0946\n",
      "Epoch 46 (0), 179.1 sec >> obj loss: 1.7524, task at 10: 0.1052\n",
      "Epoch 47 (0), 179.07 sec >> obj loss: 1.6464, task at 10: 0.1003\n",
      "Epoch 48 (0), 178.99 sec >> obj loss: 1.6025, task at 10: 0.1027\n",
      "Epoch 49 (0), 178.97 sec >> obj loss: 1.5964, task at 10: 0.1028\n",
      "Epoch 50 (0), 179.1 sec >> obj loss: 1.5909, task at 10: 0.0991\n",
      "Epoch 51 (1), 179.12 sec >> obj loss: 1.5933, task at 10: 0.0992\n",
      "Epoch 52 (0), 179.12 sec >> obj loss: 1.5806, task at 10: 0.0987\n",
      "Epoch 53 (1), 179.4 sec >> obj loss: 1.5958, task at 10: 0.0975\n",
      "Epoch 54 (2), 178.98 sec >> obj loss: 1.5940, task at 10: 0.1002\n",
      "Epoch 55 (3), 178.93 sec >> obj loss: 1.6008, task at 10: 0.0995\n",
      "Epoch 56 (4), 179.01 sec >> obj loss: 1.6068, task at 10: 0.0995\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "tasks = ['100', '70', '40', '10']\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-4\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 4\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # task-wise loss aggregation\n",
    "        obj = tf.add(tf.multiply(obj, 1.0-alpha), tf.multiply(new_obj, alpha))\n",
    "        \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.max\n",
    "            val_accu = val_accu/pval.max\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "\n",
    "#             epoch_summary = sess.run([vgg16.summary_op],\n",
    "#                                     feed_dict={vgg16.x: Xtest,\n",
    "#                                                vgg16.y: Ytest})\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556d42e522ca4369bc64224ae89e2efd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adf873ed92143f4a6b1a8955202d1a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 61.73 sec >> obj loss: 0.1561, task at 100: 0.1100\n",
      "Epoch 2 (0), 60.05 sec >> obj loss: 0.1114, task at 100: 0.1230\n",
      "Epoch 3 (0), 59.91 sec >> obj loss: 0.0935, task at 100: 0.1283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3e2edaeaac9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m                 loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n\u001b[1;32m     82\u001b[0m                                     feed_dict={vgg16.x: Xtest[st:ed,:],\n\u001b[0;32m---> 83\u001b[0;31m                                                vgg16.y: Ytest[st:ed,:]})\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mval_accu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_path = os.path.join('save/', 'model.ckpt')\n",
    "\n",
    "tasks = ['100', '70', '40', '10']\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-4\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 4\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # task-wise loss aggregation\n",
    "        obj = tf.add(tf.multiply(obj, 1.0-alpha), tf.multiply(new_obj, alpha))\n",
    "        \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/200)):\n",
    "                st = i*200\n",
    "                ed = (i+1)*200\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.max\n",
    "            val_accu = val_accu/pval.max\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "\n",
    "#             epoch_summary = sess.run([vgg16.summary_op],\n",
    "#                                     feed_dict={vgg16.x: Xtest,\n",
    "#                                                vgg16.y: Ytest})\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "accu_log = []\n",
    "loss_log = []\n",
    "tag_log = []\n",
    "counter = 0\n",
    "for e in tf.train.summary_iterator(\"log/events.out.tfevents.1516590873.theta1\"):\n",
    "    counter += 1\n",
    "    for v in e.summary.value:\n",
    "        if 'accu' in v.tag:\n",
    "            tag_log.append(str(counter-3)+\"/\"+v.tag)\n",
    "            accu_log.append(v.simple_value)\n",
    "        if 'loss' in v.tag:\n",
    "            loss_log.append(v.simple_value)\n",
    "\n",
    "output = pd.DataFrame.from_dict({'tag':tag_log, 'loss':loss_log, 'accu':accu_log})\n",
    "output.to_csv(\"output.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
