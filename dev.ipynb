{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_MEAN = [123.68, 116.779, 103.939] # [R, G, B]\n",
    "\n",
    "class vgg16:\n",
    "    def __init__(self, vgg16_npy_path=None):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        if vgg16_npy_path is None:\n",
    "            path = inspect.getfile(Vgg16)\n",
    "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "            path = os.path.join(path, \"vgg16_weights.npz\")\n",
    "            vgg16_npy_path = path\n",
    "            print(path)\n",
    "        \n",
    "        self.data_dict = np.load(vgg16_npy_path)\n",
    "        print(\"npy file loaded\")\n",
    "        \n",
    "        self.B, self.H, self.W, self.C = 32, 32, 32, 3\n",
    "        self.classes = 10\n",
    "        \n",
    "        # dictionary for operation\n",
    "        self.prob_dict = {}\n",
    "        self.loss_dict = {}\n",
    "        self.accu_dict = {}\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [self.B, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [self.B, self.classes])\n",
    "\n",
    "    def build(self, dp, prof_type):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "        self.dp = dp \n",
    "        print(\"Will optimize at DP=\", self.dp)\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        rgb_scaled = self.x * 255.0\n",
    "\n",
    "        # normalization\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert   red.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert  blue.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        self.x = tf.concat(axis=3, values=[\n",
    "              red - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "             blue - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "        with tf.variable_scope(\"vgg16\"):\n",
    "        \n",
    "            self.conv1_1_W, self.conv1_1_b = self.get_conv_filter(\"conv1_1\"), self.get_bias(\"conv1_1\")\n",
    "            self.conv1_2_W, self.conv1_2_b = self.get_conv_filter(\"conv1_2\"), self.get_bias(\"conv1_2\")\n",
    "\n",
    "            self.conv2_1_W, self.conv2_1_b = self.get_conv_filter(\"conv2_1\"), self.get_bias(\"conv2_1\")\n",
    "            self.conv2_2_W, self.conv2_2_b = self.get_conv_filter(\"conv2_2\"), self.get_bias(\"conv2_2\")\n",
    "\n",
    "            self.conv3_1_W, self.conv3_1_b = self.get_conv_filter(\"conv3_1\"), self.get_bias(\"conv3_1\")\n",
    "            self.conv3_2_W, self.conv3_2_b = self.get_conv_filter(\"conv3_2\"), self.get_bias(\"conv3_2\")\n",
    "            self.conv3_3_W, self.conv3_3_b = self.get_conv_filter(\"conv3_3\"), self.get_bias(\"conv3_3\")\n",
    "\n",
    "            self.conv4_1_W, self.conv4_1_b = self.get_conv_filter(\"conv4_1\"), self.get_bias(\"conv4_1\")\n",
    "            self.conv4_2_W, self.conv4_2_b = self.get_conv_filter(\"conv4_2\"), self.get_bias(\"conv4_2\")\n",
    "            self.conv4_3_W, self.conv4_3_b = self.get_conv_filter(\"conv4_3\"), self.get_bias(\"conv4_3\")\n",
    "\n",
    "            self.conv5_1_W, self.conv5_1_b = self.get_conv_filter(\"conv5_1\"), self.get_bias(\"conv5_1\")\n",
    "            self.conv5_2_W, self.conv5_2_b = self.get_conv_filter(\"conv5_2\"), self.get_bias(\"conv5_2\")\n",
    "            self.conv5_3_W, self.conv5_3_b = self.get_conv_filter(\"conv5_3\"), self.get_bias(\"conv5_3\")\n",
    "\n",
    "            self.fc_1_W = tf.get_variable(name=\"fc_1_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_1_b = tf.get_variable(name=\"fc_1_b\", shape=(512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "            self.fc_2_W = tf.get_variable(name=\"fc_2_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_2_b = tf.get_variable(name=\"fc_2_b\", shape=(512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "            self.fc_3_W = tf.get_variable(name=\"fc_3_W\", shape=(512, 10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_3_b = tf.get_variable(name=\"fc_3_b\", shape=(10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        # for loop for dp, assign prof_type\n",
    "        for dp_i in dp:\n",
    "            with tf.name_scope(str(int(dp_i*100))):\n",
    "                conv1_1 = self.idp_conv_layer( self.x, \"conv1_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv1_2 = self.idp_conv_layer(conv1_1, \"conv1_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "                conv2_1 = self.idp_conv_layer(  pool1, \"conv2_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv2_2 = self.idp_conv_layer(conv2_1, \"conv2_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "                conv3_1 = self.idp_conv_layer(  pool2, \"conv3_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_2 = self.idp_conv_layer(conv3_1, \"conv3_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_3 = self.idp_conv_layer(conv3_2, \"conv3_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "                conv4_1 = self.idp_conv_layer(  pool3, \"conv4_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_2 = self.idp_conv_layer(conv4_1, \"conv4_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_3 = self.idp_conv_layer(conv4_2, \"conv4_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool4 = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "                conv5_1 = self.idp_conv_layer(  pool4, \"conv5_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_2 = self.idp_conv_layer(conv5_1, \"conv5_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_3 = self.idp_conv_layer(conv5_2, \"conv5_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "                fc_1 = self.fc_layer(pool5, 'fc_1')\n",
    "                fc_1 = tf.nn.relu(fc_1)\n",
    "                fc_2 = self.fc_layer(fc_1, 'fc_2')\n",
    "                fc_2 = tf.nn.relu(fc_2)\n",
    "                \n",
    "                logits = tf.nn.bias_add(tf.matmul( fc_2, self.fc_3_W), self.fc_3_b)\n",
    "                prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "                \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "                loss = tf.reduce_mean(cross_entropy)\n",
    "                accuracy = tf.mean(tf.equal(x=tf.argmax(logits, 1), y=tf.argmax(self.y, 1)))\n",
    "                \n",
    "                self.prob_dict[str(int(dp_i*100))] = prob\n",
    "                self.loss_dict[str(int(dp_i*100))] = loss\n",
    "                self.accu_dict[str(int(dp_i*100))] = accuracy\n",
    "                \n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def idp_conv_layer(self, bottom, name, dp, prof_type, gamma_trainable = False):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "            \n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "        \n",
    "            # get profile\n",
    "            profile = self.get_profile(O, prof_type)\n",
    "            \n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32'))\n",
    "            if len(profile) == len(mask):\n",
    "                profile *= mask\n",
    "            else:\n",
    "                raise ValueError(\"profile and mask must have the same shape.\")\n",
    "\n",
    "            # create a profile coefficient, gamma\n",
    "            filter_profile = np.stack([profile for i in range(H*W*C)])\n",
    "            filter_profile = np.reshape(filter_profile, newshape=(H, W, C, O))\n",
    "\n",
    "            gamma_W = tf.Variable(initial_value=filter_profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "            gamma_b = tf.Variable(initial_value=profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "\n",
    "            # IDP conv2d output\n",
    "            conv_filter = tf.multiply(conv_filter, gamma_W)\n",
    "            conv_biases = tf.multiply(conv_biases, gamma_b)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            \n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "            \n",
    "            relu = tf.nn.relu(conv)\n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.name_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            \n",
    "            with tf.variable_scope(\"vgg16\",reuse=True):\n",
    "                weights = tf.get_variable(name=name+\"_W\")\n",
    "                biases = tf.get_variable(name=name+\"_b\")\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_W\"], name=name+\"_W\")\n",
    "    def get_bias(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_b\"], name=name+\"_b\")\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name+\"_W\"], name=name+\"_W\")\n",
    "\n",
    "    def get_profile(self, C, prof_type):\n",
    "        def half_exp(n, k=1, dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - n_ones\n",
    "            return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "        if prof_type == \"linear\":\n",
    "            profile = np.linspace(1.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "        elif prof_type == \"all-one\":\n",
    "            profile = np.ones(C, dtype='float32')\n",
    "        elif prof_type == \"half-exp\":\n",
    "            profile = half_exp(C, 2.0)\n",
    "        elif prof_type == \"harmonic\":\n",
    "            profile = np.array(1.0/(np.arange(C)+1))\n",
    "        else:\n",
    "            raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "        return profile\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy file loaded\n"
     ]
    }
   ],
   "source": [
    "vgg16 = vgg16(\"vgg16_weights.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will optimize at DP= [0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001, 1.0]\n",
      "build model started\n",
      "build model finished: 7s\n"
     ]
    }
   ],
   "source": [
    "vgg16.build(dp=[(i+1)*0.05 for i in range(1,20)],prof_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = tf.add(vgg16.loss_dict['100'],vgg16.loss_dict['70'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = tf.add(obj,vgg16.loss_dict['50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted model: 0.1875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(vgg16.fc_3_b))\n",
    "    for i in range(1):\n",
    "        sess.run(train_op, feed_dict={vgg16.x: Xtrain[:32,:,:,:],\n",
    "                                         vgg16.y: Ytrain[:32,:]})\n",
    "    \n",
    "    perf = sess.run(vgg16.accu_dict['100'],\n",
    "                       feed_dict={vgg16.x: Xtrain[:32,:,:,:],\n",
    "                                  vgg16.y: Ytrain[:32,:]})\n",
    "    print(sess.run(vgg16.fc_3_b))\n",
    "    print(\"Predicted model: {a:.4f}\".format(a=np.mean(perf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = tf.Session()\n",
    "s1.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0172167   0.03313492  0.04479698  0.01052409 -0.0890021   0.04248823\n",
      " -0.06832712 -0.14019343 -0.166647   -0.07080839]\n"
     ]
    }
   ],
   "source": [
    "print(s1.run(vgg16.fc_3_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "########################################################################\n",
    "#\n",
    "# Functions for downloading the CIFAR-10 data-set from the internet\n",
    "# and loading it into memory.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "# Usage:\n",
    "# 1) Set the variable data_path with the desired storage path.\n",
    "# 2) Call maybe_download_and_extract() to download the data-set\n",
    "#    if it is not already located in the given data_path.\n",
    "# 3) Call load_class_names() to get an array of the class-names.\n",
    "# 4) Call load_training_data() and load_test_data() to get\n",
    "#    the images, class-numbers and one-hot encoded class-labels\n",
    "#    for the training-set and test-set.\n",
    "# 5) Use the returned data in your own program.\n",
    "#\n",
    "# Format:\n",
    "# The images for the training- and test-sets are returned as 4-dim numpy\n",
    "# arrays each with the shape: [image_number, height, width, channel]\n",
    "# where the individual pixels are floats between 0.0 and 1.0.\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "#import download\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# Directory where you want to download and save the data-set.\n",
    "# Set this before you start calling any of the functions below.\n",
    "data_path = \"/Users/chunmingchang/data/\"\n",
    "\n",
    "# URL for the data-set on the internet.\n",
    "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "########################################################################\n",
    "# Various constants for the size of the images.\n",
    "# Use these constants in your own program.\n",
    "\n",
    "# Width and height of each image.\n",
    "img_size = 32\n",
    "\n",
    "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "num_channels = 3\n",
    "\n",
    "# Length of an image when flattened to a 1-dim array.\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 10\n",
    "\n",
    "########################################################################\n",
    "# Various constants used to allocate arrays of the correct size.\n",
    "\n",
    "# Number of files for the training-set.\n",
    "_num_files_train = 5\n",
    "\n",
    "# Number of images for each batch-file in the training-set.\n",
    "_images_per_file = 10000\n",
    "\n",
    "# Total number of images in the training-set.\n",
    "# This is used to pre-allocate arrays for efficiency.\n",
    "_num_images_train = _num_files_train * _images_per_file\n",
    "\n",
    "########################################################################\n",
    "# \n",
    "def _one_hot_encoded(class_numbers, num_classes=None):\n",
    "    \"\"\"\n",
    "    Generate the One-Hot encoded class-labels from an array of integers.\n",
    "    For example, if class_number=2 and num_classes=4 then\n",
    "    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
    "    :param class_numbers:\n",
    "        Array of integers with class-numbers.\n",
    "        Assume the integers are from zero to num_classes-1 inclusive.\n",
    "    :param num_classes:\n",
    "        Number of classes. If None then use max(class_numbers)+1.\n",
    "    :return:\n",
    "        2-dim array of shape: [len(class_numbers), num_classes]\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the number of classes if None is provided.\n",
    "    # Assumes the lowest class-number is zero.\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(class_numbers) + 1\n",
    "\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Private functions for downloading, unpacking and loading data-files.\n",
    "\n",
    "def _get_file_path(filename=\"\"):\n",
    "    \"\"\"\n",
    "    Return the full path of a data-file for the data-set.\n",
    "\n",
    "    If filename==\"\" then return the directory of the files.\n",
    "    \"\"\"\n",
    "\n",
    "    return os.path.join(data_path, \"cifar-10-batches-py/\", filename)\n",
    "\n",
    "\n",
    "def _unpickle(filename):\n",
    "    \"\"\"\n",
    "    Unpickle the given file and return the data.\n",
    "\n",
    "    Note that the appropriate dir-name is prepended the filename.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create full path for the file.\n",
    "    file_path = _get_file_path(filename)\n",
    "\n",
    "    print(\"Loading data: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _convert_images(raw):\n",
    "    \"\"\"\n",
    "    Convert images from the CIFAR-10 format and\n",
    "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "    where the pixels are floats between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the raw images from the data-files to floating-points.\n",
    "    raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "    # Reshape the array to 4-dimensions.\n",
    "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
    "\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def _load_data(filename):\n",
    "    \"\"\"\n",
    "    Load a pickled data-file from the CIFAR-10 data-set\n",
    "    and return the converted images (see above) and the class-number\n",
    "    for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pickled data-file.\n",
    "    data = _unpickle(filename)\n",
    "\n",
    "    # Get the raw images.\n",
    "    raw_images = data[b'data']\n",
    "\n",
    "    # Get the class-numbers for each image. Convert to numpy-array.\n",
    "    cls = np.array(data[b'labels'])\n",
    "\n",
    "    # Convert the images.\n",
    "    images = _convert_images(raw_images)\n",
    "\n",
    "    return images, cls\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Public functions that you may call to download the data-set from\n",
    "# the internet and load the data into memory.\n",
    "\n",
    "\n",
    "# def maybe_download_and_extract():\n",
    "#     \"\"\"\n",
    "#     Download and extract the CIFAR-10 data-set if it doesn't already exist\n",
    "#     in data_path (set this variable first to the desired path).\n",
    "#     \"\"\"\n",
    "\n",
    "#     download.maybe_download_and_extract(url=data_url, download_dir=data_path)\n",
    "\n",
    "\n",
    "def load_class_names():\n",
    "    \"\"\"\n",
    "    Load the names for the classes in the CIFAR-10 data-set.\n",
    "\n",
    "    Returns a list with the names. Example: names[3] is the name\n",
    "    associated with class-number 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the class-names from the pickled file.\n",
    "    raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
    "\n",
    "    # Convert from binary strings.\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "\n",
    "    return names\n",
    "\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"\n",
    "    Load all the training-data for the CIFAR-10 data-set.\n",
    "\n",
    "    The data-set is split into 5 data-files which are merged here.\n",
    "\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n",
    "    cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
    "\n",
    "    # Begin-index for the current batch.\n",
    "    begin = 0\n",
    "\n",
    "    # For each data-file.\n",
    "    for i in range(_num_files_train):\n",
    "        # Load the images and class-numbers from the data-file.\n",
    "        images_batch, cls_batch = _load_data(filename=\"data_batch_\" + str(i + 1))\n",
    "\n",
    "        # Number of images in this batch.\n",
    "        num_images = len(images_batch)\n",
    "\n",
    "        # End-index for the current batch.\n",
    "        end = begin + num_images\n",
    "\n",
    "        # Store the images into the array.\n",
    "        images[begin:end, :] = images_batch\n",
    "\n",
    "        # Store the class-numbers into the array.\n",
    "        cls[begin:end] = cls_batch\n",
    "\n",
    "        # The begin-index for the next batch is the current end-index.\n",
    "        begin = end\n",
    "\n",
    "    return images, _one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Load all the test-data for the CIFAR-10 data-set.\n",
    "\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    images, cls = _load_data(filename=\"test_batch\")\n",
    "\n",
    "    return images, _one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: /Users/chunmingchang/data/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu100, accu70, accu50 = sess.run([vgg16.accu_dict['100'], \n",
    "                                                    vgg16.accu_dict['70'],\n",
    "                                                    vgg16.accu_dict['50']],\n",
    "                                                    feed_dict={vgg16.x: Xtrain[:32,:,:,:],\n",
    "                                                               vgg16.y: Ytrain[:32,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load vgg16.py\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops.gen_nn_ops import *\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def my_profile(C, prof_type):\n",
    "    def half_exp(n, k=1, dtype='float32'):\n",
    "        n_ones = int(n/2)\n",
    "        n_other = n - n_ones\n",
    "        return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "    if prof_type == \"linear\":\n",
    "        profile = np.linspace(1.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "    elif prof_type == \"all-one\":\n",
    "        profile = np.ones(C, dtype='float32')\n",
    "    elif prof_type == \"half-exp\":\n",
    "        profile = half_exp(C, 2.0)\n",
    "    elif prof_type == \"harmonic\":\n",
    "        profile = np.array(1.0/(np.arange(C)+1))\n",
    "    else:\n",
    "        raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "    return profile\n",
    "\n",
    "def idp_conv2d(input, filter, strides, padding,\n",
    "               use_cudnn_on_gpu=True, data_format='NHWC',\n",
    "               name=None, prof_type=None, dp=1.0):\n",
    "    with ops.name_scope(name, \"idp_convolution\", [input, filter]) as scope:\n",
    "        if not (data_format == \"NHWC\" or data_format == \"NCHW\"):\n",
    "            raise ValueError(\"data_format must be \\\"NHWC\\\" or \\\"NCHW\\\".\")\n",
    "        \n",
    "        # do conv2d\n",
    "        conv2d_res = gen_nn_ops.conv2d(input, filter, strides, padding,\n",
    "                                       use_cudnn_on_gpu=True,\n",
    "                                       data_format='NHWC',\n",
    "                                       name=None)\n",
    "        B,H,W,C = conv2d_res.get_shape().as_list()\n",
    "        \n",
    "        # get profile\n",
    "        profile = my_profile(C, prof_type)\n",
    "        # tensor_profile = tf.get_variable(initializer=profile,name=\"tensor_profile\",dtype='float32')\n",
    "\n",
    "        # create a mask determined by the dot product percentage\n",
    "        n1 = int(C * dp)\n",
    "        n0 = C - n1\n",
    "        mask = np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32'))\n",
    "        if len(profile) == len(mask):\n",
    "            profile *= mask\n",
    "        else:\n",
    "            raise ValueError(\"profile and mask must have the same shape.\")\n",
    "\n",
    "        # create a profile coefficient, gamma\n",
    "        conv2d_profile = np.stack([profile for i in range(B*H*W)])\n",
    "        conv2d_profile = np.reshape(conv2d_profile, newshape=(B, H, W, C))\n",
    "        \n",
    "        gamma = tf.get_variable(initializer=conv2d_profile, name=\"gamma\"+str(dp*100))\n",
    "        \n",
    "        # IDP conv2d output\n",
    "        idp_conv2d_res = tf.multiply(conv2d_res, gamma, name=\"idp\"+str(dp*100))\n",
    "        \n",
    "        return idp_conv2d_res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
