{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# VGG_MEAN = [123.68, 116.779, 103.939] # [R, G, B]\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] # [R, G, B]\n",
    "class VGG16:\n",
    "    def __init__(self, vgg16_npy_path):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        \n",
    "        # load pre-trained weights    \n",
    "        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "        print(\"npy file loaded\")\n",
    "        \n",
    "        # input information\n",
    "        self.H, self.W, self.C = 32, 32, 3\n",
    "        self.classes = 10\n",
    "        \n",
    "        # operation dictionary\n",
    "        self.prob_dict = {}\n",
    "        self.loss_dict = {}\n",
    "        self.accu_dict = {}\n",
    "        \n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.classes])\n",
    "\n",
    "    def build(self, dp, prof_type):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dp = dp \n",
    "        print(\"Will optimize at DP=\", self.dp)\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        rgb_scaled = self.x * 255.0\n",
    "\n",
    "        # normalize input by VGG_MEAN\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert   red.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert  blue.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        self.x = tf.concat(axis=3, values=[\n",
    "              blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "             red - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "        # declare and initialize the weights of VGG16\n",
    "        with tf.variable_scope(\"VGG16\"):\n",
    "        \n",
    "            self.conv1_1_W, self.conv1_1_b = self.get_conv_filter(\"conv1_1\"), self.get_bias(\"conv1_1\")\n",
    "            self.conv1_2_W, self.conv1_2_b = self.get_conv_filter(\"conv1_2\"), self.get_bias(\"conv1_2\")\n",
    "\n",
    "            self.conv2_1_W, self.conv2_1_b = self.get_conv_filter(\"conv2_1\"), self.get_bias(\"conv2_1\")\n",
    "            self.conv2_2_W, self.conv2_2_b = self.get_conv_filter(\"conv2_2\"), self.get_bias(\"conv2_2\")\n",
    "\n",
    "            self.conv3_1_W, self.conv3_1_b = self.get_conv_filter(\"conv3_1\"), self.get_bias(\"conv3_1\")\n",
    "            self.conv3_2_W, self.conv3_2_b = self.get_conv_filter(\"conv3_2\"), self.get_bias(\"conv3_2\")\n",
    "            self.conv3_3_W, self.conv3_3_b = self.get_conv_filter(\"conv3_3\"), self.get_bias(\"conv3_3\")\n",
    "\n",
    "            self.conv4_1_W, self.conv4_1_b = self.get_conv_filter(\"conv4_1\"), self.get_bias(\"conv4_1\")\n",
    "            self.conv4_2_W, self.conv4_2_b = self.get_conv_filter(\"conv4_2\"), self.get_bias(\"conv4_2\")\n",
    "            self.conv4_3_W, self.conv4_3_b = self.get_conv_filter(\"conv4_3\"), self.get_bias(\"conv4_3\")\n",
    "\n",
    "            self.conv5_1_W, self.conv5_1_b = self.get_conv_filter(\"conv5_1\"), self.get_bias(\"conv5_1\")\n",
    "            self.conv5_2_W, self.conv5_2_b = self.get_conv_filter(\"conv5_2\"), self.get_bias(\"conv5_2\")\n",
    "            self.conv5_3_W, self.conv5_3_b = self.get_conv_filter(\"conv5_3\"), self.get_bias(\"conv5_3\")\n",
    "\n",
    "            self.fc_1_W = tf.get_variable(name=\"fc_1_W\", shape=(512, 512), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_1_b = tf.get_variable(name=\"fc_1_b\", shape=(512), initializer=tf.ones_initializer(), dtype=tf.float32)\n",
    "\n",
    "            self.fc_2_W = tf.get_variable(name=\"fc_2_W\", shape=(512, 10), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), dtype=tf.float32)\n",
    "            self.fc_2_b = tf.get_variable(name=\"fc_2_b\", shape=(10), initializer=tf.ones_initializer(), dtype=tf.float32)\n",
    "        \n",
    "        # create operations at every dot product percentages\n",
    "        for dp_i in dp:\n",
    "            with tf.name_scope(str(int(dp_i*100))):\n",
    "                conv1_1 = self.idp_conv_layer( self.x, \"conv1_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv1_2 = self.idp_conv_layer(conv1_1, \"conv1_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "                conv2_1 = self.idp_conv_layer(  pool1, \"conv2_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv2_2 = self.idp_conv_layer(conv2_1, \"conv2_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "                conv3_1 = self.idp_conv_layer(  pool2, \"conv3_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_2 = self.idp_conv_layer(conv3_1, \"conv3_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv3_3 = self.idp_conv_layer(conv3_2, \"conv3_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "                conv4_1 = self.idp_conv_layer(  pool3, \"conv4_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_2 = self.idp_conv_layer(conv4_1, \"conv4_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv4_3 = self.idp_conv_layer(conv4_2, \"conv4_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool4 = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "                conv5_1 = self.idp_conv_layer(  pool4, \"conv5_1\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_2 = self.idp_conv_layer(conv5_1, \"conv5_2\", dp_i, prof_type, gamma_trainable=False)\n",
    "                conv5_3 = self.idp_conv_layer(conv5_2, \"conv5_3\", dp_i, prof_type, gamma_trainable=False)\n",
    "                pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "                fc_1 = self.fc_layer(pool5, 'fc_1')\n",
    "                fc_1 = tf.nn.dropout(fc_1, keep_prob=0.5)\n",
    "                fc_1 = tf.nn.relu(fc_1)\n",
    "                \n",
    "                logits = tf.nn.bias_add(tf.matmul( fc_1, self.fc_2_W), self.fc_2_b)\n",
    "                prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "                \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "                loss = tf.reduce_mean(cross_entropy)\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(x=tf.argmax(logits, 1), y=tf.argmax(self.y, 1)),tf.float32))\n",
    "                \n",
    "                self.prob_dict[str(int(dp_i*100))] = prob\n",
    "                self.loss_dict[str(int(dp_i*100))] = loss\n",
    "                self.accu_dict[str(int(dp_i*100))] = accuracy\n",
    "                \n",
    "                tf.summary.scalar(name=\"accu_at_\"+str(int(dp_i*100)), tensor=accuracy)\n",
    "                tf.summary.scalar(name=\"loss_at_\"+str(int(dp_i*100)), tensor=loss)\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def idp_conv_layer(self, bottom, name, dp, prof_type, gamma_trainable = False):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(\"VGG16\",reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "            \n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "        \n",
    "            # get profile\n",
    "            profile = self.get_profile(O, prof_type)\n",
    "            \n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32'))\n",
    "            if len(profile) == len(mask):\n",
    "                profile *= mask\n",
    "            else:\n",
    "                raise ValueError(\"profile and mask must have the same shape.\")\n",
    "\n",
    "            # create a profile coefficient, gamma\n",
    "            filter_profile = np.stack([profile for i in range(H*W*C)])\n",
    "            filter_profile = np.reshape(filter_profile, newshape=(H, W, C, O))\n",
    "            \n",
    "            # gamma in use\n",
    "            gamma_W = tf.Variable(initial_value=filter_profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "            gamma_b = tf.Variable(initial_value=profile, name=name+\"_gamma_W_\"+str(int(dp*100)), trainable=gamma_trainable)\n",
    "\n",
    "            # IDP conv2d output\n",
    "            conv_filter = tf.multiply(conv_filter, gamma_W)\n",
    "            conv_biases = tf.multiply(conv_biases, gamma_b)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "            relu = tf.nn.relu(conv)\n",
    "            \n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.name_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            \n",
    "            with tf.variable_scope(\"VGG16\",reuse=True):\n",
    "                weights = tf.get_variable(name=name+\"_W\")\n",
    "                biases = tf.get_variable(name=name+\"_b\")\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "    def get_bias(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][1], name=name+\"_b\")\n",
    "#     def get_fc_weight(self, name):\n",
    "#         return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "\n",
    "    def get_profile(self, C, prof_type):\n",
    "        def half_exp(n, k=1, dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - n_ones\n",
    "            return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "        if prof_type == \"linear\":\n",
    "            profile = np.linspace(2.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "        elif prof_type == \"all-one\":\n",
    "            profile = np.ones(C, dtype='float32')\n",
    "        elif prof_type == \"half-exp\":\n",
    "            profile = half_exp(C, 2.0)\n",
    "        elif prof_type == \"harmonic\":\n",
    "            profile = np.array(1.0/(np.arange(C)+1))\n",
    "        else:\n",
    "            raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "        return profile\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Functions for downloading the CIFAR-10 data-set from the internet\n",
    "# and loading it into memory.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "# Usage:\n",
    "# 1) Set the variable data_path with the desired storage path.\n",
    "# 2) Call maybe_download_and_extract() to download the data-set\n",
    "#    if it is not already located in the given data_path.\n",
    "# 3) Call load_class_names() to get an array of the class-names.\n",
    "# 4) Call load_training_data() and load_test_data() to get\n",
    "#    the images, class-numbers and one-hot encoded class-labels\n",
    "#    for the training-set and test-set.\n",
    "# 5) Use the returned data in your own program.\n",
    "#\n",
    "# Format:\n",
    "# The images for the training- and test-sets are returned as 4-dim numpy\n",
    "# arrays each with the shape: [image_number, height, width, channel]\n",
    "# where the individual pixels are floats between 0.0 and 1.0.\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class CIFAR10(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        ########################################################################\n",
    "        # Directory where you want to download and save the data-set.\n",
    "        # Set this before you start calling any of the functions below.\n",
    "        self.data_path = \"/home/cmchang/IDP_CNN/data/\"\n",
    "\n",
    "        # URL for the data-set on the internet.\n",
    "        self.data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "        ########################################################################\n",
    "        # Various constants for the size of the images.\n",
    "        # Use these constants in your own program.\n",
    "\n",
    "        # Width and height of each image.\n",
    "        self.img_size = 32\n",
    "\n",
    "        # Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "        self.num_channels = 3\n",
    "\n",
    "        # Length of an image when flattened to a 1-dim array.\n",
    "        self.img_size_flat = self.img_size * self.img_size * self.num_channels\n",
    "\n",
    "        # Number of classes.\n",
    "        self.num_classes = 10\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "    def _one_hot_encoded(self, class_numbers, num_classes=None):\n",
    "        \"\"\"\n",
    "        Generate the One-Hot encoded class-labels from an array of integers.\n",
    "        For example, if class_number=2 and num_classes=4 then\n",
    "        the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
    "        :param class_numbers:\n",
    "            Array of integers with class-numbers.\n",
    "            Assume the integers are from zero to num_classes-1 inclusive.\n",
    "        :param num_classes:\n",
    "            Number of classes. If None then use max(class_numbers)+1.\n",
    "        :return:\n",
    "            2-dim array of shape: [len(class_numbers), num_classes]\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the number of classes if None is provided.\n",
    "        # Assumes the lowest class-number is zero.\n",
    "        if num_classes is None:\n",
    "            num_classes = np.max(class_numbers) + 1\n",
    "\n",
    "        return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "    ########################################################################\n",
    "    # Private functions for downloading, unpacking and loading data-files.\n",
    "\n",
    "    def _get_file_path(self, filename=\"\"):\n",
    "        \"\"\"\n",
    "        Return the full path of a data-file for the data-set.\n",
    "\n",
    "        If filename==\"\" then return the directory of the files.\n",
    "        \"\"\"\n",
    "\n",
    "        return os.path.join(self.data_path, \"cifar-10-batches-py/\", filename)\n",
    "\n",
    "\n",
    "    def _unpickle(self, filename):\n",
    "        \"\"\"\n",
    "        Unpickle the given file and return the data.\n",
    "\n",
    "        Note that the appropriate dir-name is prepended the filename.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create full path for the file.\n",
    "        file_path = self._get_file_path(filename)\n",
    "\n",
    "        print(\"Loading data: \" + file_path)\n",
    "\n",
    "        with open(file_path, mode='rb') as file:\n",
    "            # In Python 3.X it is important to set the encoding,\n",
    "            # otherwise an exception is raised here.\n",
    "            data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def _convert_images(self, raw):\n",
    "        \"\"\"\n",
    "        Convert images from the CIFAR-10 format and\n",
    "        return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "        where the pixels are floats between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the raw images from the data-files to floating-points.\n",
    "        raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "        # Reshape the array to 4-dimensions.\n",
    "        images = raw_float.reshape([-1, self.num_channels, self.img_size, self.img_size])\n",
    "\n",
    "        # Reorder the indices of the array.\n",
    "        images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "    def _load_data(self, filename):\n",
    "        \"\"\"\n",
    "        Load a pickled data-file from the CIFAR-10 data-set\n",
    "        and return the converted images (see above) and the class-number\n",
    "        for each image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the pickled data-file.\n",
    "        data = self._unpickle(filename)\n",
    "\n",
    "        # Get the raw images.\n",
    "        raw_images = data[b'data']\n",
    "\n",
    "        # Get the class-numbers for each image. Convert to numpy-array.\n",
    "        cls = np.array(data[b'labels'])\n",
    "\n",
    "        # Convert the images.\n",
    "        images = self._convert_images(raw_images)\n",
    "\n",
    "        return images, cls\n",
    "\n",
    "    ########################################################################\n",
    "    # Public functions that you may call to download the data-set from\n",
    "    # the internet and load the data into memory.\n",
    "\n",
    "    def load_class_names(self):\n",
    "        \"\"\"\n",
    "        Load the names for the classes in the CIFAR-10 data-set.\n",
    "\n",
    "        Returns a list with the names. Example: names[3] is the name\n",
    "        associated with class-number 3.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the class-names from the pickled file.\n",
    "        raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
    "\n",
    "        # Convert from binary strings.\n",
    "        names = [x.decode('utf-8') for x in raw]\n",
    "\n",
    "        return names\n",
    "\n",
    "\n",
    "    def load_training_data(self):\n",
    "        \"\"\"\n",
    "        Load all the training-data for the CIFAR-10 data-set.\n",
    "\n",
    "        The data-set is split into 5 data-files which are merged here.\n",
    "\n",
    "        Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "        \"\"\"\n",
    "        # Number of files for the training-set.\n",
    "        _num_files_train = 5\n",
    "\n",
    "        # Number of images for each batch-file in the training-set.\n",
    "        _images_per_file = 10000\n",
    "\n",
    "        # Total number of images in the training-set.\n",
    "        # This is used to pre-allocate arrays for efficiency.\n",
    "        _num_images_train = _num_files_train * _images_per_file\n",
    "        \n",
    "        \n",
    "        # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "        images = np.zeros(shape=[_num_images_train, self.img_size, self.img_size, self.num_channels], dtype=float)\n",
    "        cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
    "\n",
    "        # Begin-index for the current batch.\n",
    "        begin = 0\n",
    "\n",
    "        # For each data-file.\n",
    "        for i in range(_num_files_train):\n",
    "            # Load the images and class-numbers from the data-file.\n",
    "            images_batch, cls_batch = self._load_data(filename=\"data_batch_\" + str(i + 1))\n",
    "\n",
    "            # Number of images in this batch.\n",
    "            num_images = len(images_batch)\n",
    "\n",
    "            # End-index for the current batch.\n",
    "            end = begin + num_images\n",
    "\n",
    "            # Store the images into the array.\n",
    "            images[begin:end, :] = images_batch\n",
    "\n",
    "            # Store the class-numbers into the array.\n",
    "            cls[begin:end] = cls_batch\n",
    "\n",
    "            # The begin-index for the next batch is the current end-index.\n",
    "            begin = end\n",
    "\n",
    "        return images, self._one_hot_encoded(class_numbers=cls, num_classes=self.num_classes)\n",
    "\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"\n",
    "        Load all the test-data for the CIFAR-10 data-set.\n",
    "\n",
    "        Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images, cls = self._load_data(filename=\"test_batch\")\n",
    "\n",
    "        return images, self._one_hot_encoded(class_numbers=cls, num_classes=self.num_classes)\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    ########################################################################\n",
    "    #\n",
    "    # Functions for downloading and extracting data-files from the internet.\n",
    "    #\n",
    "    # Implemented in Python 3.5\n",
    "    #\n",
    "    ########################################################################\n",
    "    #\n",
    "    # This file is part of the TensorFlow Tutorials available at:\n",
    "    #\n",
    "    # https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "    #\n",
    "    # Published under the MIT License. See the file LICENSE for details.\n",
    "    #\n",
    "    # Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "    #\n",
    "    ########################################################################\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import zipfile\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "    def _print_download_progress(self, count, block_size, total_size):\n",
    "        \"\"\"\n",
    "        Function used for printing the download progress.\n",
    "        Used as a call-back function in maybe_download_and_extract().\n",
    "        \"\"\"\n",
    "\n",
    "        # Percentage completion.\n",
    "        pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "        # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "        msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "        # Print it.\n",
    "        sys.stdout.write(msg)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "    def maybe_download_and_extract(self, url, download_dir):\n",
    "        \"\"\"\n",
    "        Download and extract the data if it doesn't already exist.\n",
    "        Assumes the url is a tar-ball file.\n",
    "        :param url:\n",
    "            Internet URL for the tar-file to download.\n",
    "            Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        :param download_dir:\n",
    "            Directory where the downloaded file is saved.\n",
    "            Example: \"data/CIFAR-10/\"\n",
    "        :return:\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Filename for saving the file downloaded from the internet.\n",
    "        # Use the filename from the URL and add it to the download_dir.\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        # Check if the file already exists.\n",
    "        # If it exists then we assume it has also been extracted,\n",
    "        # otherwise we need to download and extract it now.\n",
    "        if not os.path.exists(file_path):\n",
    "            # Check if the download directory exists, otherwise create it.\n",
    "            if not os.path.exists(download_dir):\n",
    "                os.makedirs(download_dir)\n",
    "\n",
    "            # Download the file from the internet.\n",
    "            file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                      filename=file_path,\n",
    "                                                      reporthook=self._print_download_progress)\n",
    "\n",
    "            print()\n",
    "            print(\"Download finished. Extracting files.\")\n",
    "\n",
    "            if file_path.endswith(\".zip\"):\n",
    "                # Unpack the zip-file.\n",
    "                zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "            elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "                # Unpack the tar-ball.\n",
    "                tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "            print(\"Done.\")\n",
    "        else:\n",
    "            print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "\n",
    "    ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    \n",
    "    # print([str(i.name) for i in not_initialized_vars])\n",
    "    \n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"CIFAR-10\"\n",
    "save_dir = \"/data/put_data/cmchang/save_linear_no_alpha/\"\n",
    "prof_type = \"linear\"\n",
    "outputfn = \"output_linear_2.csv\"\n",
    "atp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Build VGG16 models...\n",
      "npy file loaded\n",
      "Will optimize at DP= [0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001, 1.0]\n",
      "build model started\n",
      "build model finished: 3s\n",
      "INFO:tensorflow:Restoring parameters from /data/put_data/cmchang/save_linear_no_alpha/model.ckpt-19\n",
      "Model restored /data/put_data/cmchang/save_linear_no_alpha/model.ckpt-19\n",
      "Initialized\n",
      "At DP=0.1000, accu=0.1018\n",
      "At DP=0.1500, accu=0.1012\n",
      "At DP=0.2000, accu=0.1038\n",
      "At DP=0.2500, accu=0.1020\n",
      "At DP=0.3000, accu=0.0984\n",
      "At DP=0.3500, accu=0.1128\n",
      "At DP=0.4000, accu=0.1426\n",
      "At DP=0.4500, accu=0.2242\n",
      "At DP=0.5000, accu=0.3886\n",
      "At DP=0.5500, accu=0.4970\n",
      "At DP=0.6000, accu=0.5824\n",
      "At DP=0.6500, accu=0.6776\n",
      "At DP=0.7000, accu=0.7434\n",
      "At DP=0.7500, accu=0.7410\n",
      "At DP=0.8000, accu=0.7688\n",
      "At DP=0.8500, accu=0.7702\n",
      "At DP=0.9000, accu=0.7796\n",
      "At DP=0.9500, accu=0.7832\n",
      "At DP=1.0000, accu=0.7792\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FLAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3e894abb0d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'DP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_i\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdp_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'accu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Write into %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAG' is not defined"
     ]
    }
   ],
   "source": [
    "# %load test.py\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import CIFAR10, CIFAR100\n",
    "\n",
    "\n",
    "print(\"Reading dataset...\")\n",
    "if dataset == 'CIFAR-10':\n",
    "    dataset = CIFAR10(train=False)\n",
    "elif dataset == 'CIFAR-100':\n",
    "    dataset = CIFAR100(train=False)\n",
    "else:\n",
    "    raise ValueError(\"dataset should be either CIFAR-10 or CIFAR-100.\")\n",
    "\n",
    "Xtest, Ytest = dataset.test_data, dataset.test_labels\n",
    "\n",
    "print(\"Build VGG16 models...\")\n",
    "dp = [(i+1)*0.05 for i in range(1,20)]\n",
    "vgg16 = VGG16(\"/home/cmchang/IDP_CNN/vgg16.npy\")\n",
    "vgg16.build(dp=dp, prof_type=prof_type)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if save_dir is not None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(\"Model restored %s\" % ckpt.model_checkpoint_path)\n",
    "            sess.run(tf.global_variables())\n",
    "        print(\"Initialized\")\n",
    "\n",
    "        output = []\n",
    "        for dp_i in dp:\n",
    "            accu = sess.run(vgg16.accu_dict[str(int(dp_i*100))], feed_dict={vgg16.x: Xtest[:5000,:], vgg16.y: Ytest[:5000,:]})\n",
    "            output.append(accu)\n",
    "            print(\"At DP={dp:.4f}, accu={perf:.4f}\".format(dp=dp_i, perf=accu))\n",
    "        res = pd.DataFrame.from_dict({'DP':[int(dp_i*100) for dp_i in dp],'accu':output})\n",
    "        res.to_csv(outputfn, index=False)\n",
    "        print(\"Write into %s\" % FLAG.outputfn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_1\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_2\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_3\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_4\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/data_batch_5\n",
      "Loading data: /home/cmchang/IDP_CNN/data/cifar-10-batches-py/test_batch\n",
      "npy file loaded\n",
      "Will optimize at DP= [0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001, 1.0]\n",
      "build model started\n",
      "build model finished: 3s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import VGG16\n",
    "#import CIFAR10\n",
    "\n",
    "dataset = CIFAR10()\n",
    "Xtrain, Ytrain = dataset.load_training_data()\n",
    "Xtest, Ytest = dataset.load_test_data()\n",
    "\n",
    "###\n",
    "dp = [(i+1)*0.05 for i in range(1,20)]\n",
    "#vgg16 = VGG16(\"vgg16_weights.npz\")\n",
    "vgg16 = VGG16(\"/home/cmchang/IDP_CNN/vgg16.npy\")\n",
    "vgg16.build(dp=dp, prof_type=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 200\n",
    "alpha = 0.5\n",
    "early_stop_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = vgg16.loss_dict['100']\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(obj)\n",
    "train_op = tf.train.GradientDescentOptimizer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\"log/\",sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    val_loss = sess.run(vgg16.accu_dict['100'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "                                   vgg16.y: Ytest[:5000,:]})\n",
    "    print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "    # training an epoch\n",
    "    for i in range(5):\n",
    "        st = i*batch_size\n",
    "        ed = (i+1)*batch_size\n",
    "        _, summary = sess.run([train_op, vgg16.summary_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                        vgg16.y: Ytrain[st:ed,:]})\n",
    "        summary_writer.add_summary(summary, i)\n",
    "#         # print(summary)\n",
    "#     val_loss = sess.run(vgg16.accu_dict['100'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "#     val_loss = sess.run(vgg16.accu_dict['70'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    \n",
    "#     val_loss = sess.run(vgg16.accu_dict['30'],feed_dict={vgg16.x: Xtest[:5000,:,:,:],\n",
    "#                                    vgg16.y: Ytest[:5000,:]})\n",
    "#     print(\"Predicted model: {a:.4f}\".format(a=val_loss))\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = sorted([int(dp_i*100) for dp_i in dp],reverse=True)\n",
    "tasks = [str(task) for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.random.permutation(Xtest.shape[0])[:(int(Xtest.shape[0]/2))]\n",
    "# Xtest, Ytest= Xtest[idx,:], Ytest[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24580fcb3c4b699773cb8347ca3ab5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452c60b83caa46d3823c0e2a79d453a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 78.7 sec >> obj loss: 0.8100, task at 100: 0.7217\n",
      "Epoch 2 (0), 78.28 sec >> obj loss: 0.6649, task at 100: 0.7732\n",
      "Epoch 3 (0), 78.65 sec >> obj loss: 0.5973, task at 100: 0.7961\n",
      "Epoch 4 (0), 78.59 sec >> obj loss: 0.5464, task at 100: 0.8155\n",
      "Epoch 5 (0), 78.08 sec >> obj loss: 0.5087, task at 100: 0.8330\n",
      "Epoch 6 (1), 78.47 sec >> obj loss: 0.5217, task at 100: 0.8277\n",
      "Epoch 7 (0), 78.58 sec >> obj loss: 0.4970, task at 100: 0.8417\n",
      "Epoch 8 (0), 78.3 sec >> obj loss: 0.4895, task at 100: 0.8428\n",
      "Epoch 9 (1), 78.46 sec >> obj loss: 0.5340, task at 100: 0.8340\n",
      "Epoch 10 (2), 78.45 sec >> obj loss: 0.5117, task at 100: 0.8435\n",
      "Epoch 11 (3), 78.35 sec >> obj loss: 0.5248, task at 100: 0.8476\n",
      "Epoch 12 (4), 78.47 sec >> obj loss: 0.5334, task at 100: 0.8493\n",
      "Epoch 13 (0), 112.75 sec >> obj loss: 0.7562, task at 70: 0.6721\n",
      "Epoch 14 (0), 112.76 sec >> obj loss: 0.6974, task at 70: 0.7357\n",
      "Epoch 15 (0), 112.69 sec >> obj loss: 0.6612, task at 70: 0.7669\n",
      "Epoch 16 (1), 112.72 sec >> obj loss: 0.6621, task at 70: 0.7766\n",
      "Epoch 17 (2), 112.6 sec >> obj loss: 0.6843, task at 70: 0.7930\n",
      "Epoch 18 (3), 112.61 sec >> obj loss: 0.6636, task at 70: 0.8023\n",
      "Epoch 19 (0), 112.6 sec >> obj loss: 0.6513, task at 70: 0.8095\n",
      "Epoch 20 (1), 112.66 sec >> obj loss: 0.6863, task at 70: 0.8124\n",
      "Epoch 21 (2), 112.83 sec >> obj loss: 0.6965, task at 70: 0.8228\n",
      "Epoch 22 (3), 112.71 sec >> obj loss: 0.6871, task at 70: 0.8238\n",
      "Epoch 23 (4), 112.62 sec >> obj loss: 0.7185, task at 70: 0.8184\n",
      "Epoch 24 (0), 145.7 sec >> obj loss: 1.3858, task at 40: 0.1982\n",
      "Epoch 25 (0), 145.32 sec >> obj loss: 1.3216, task at 40: 0.2665\n",
      "Epoch 26 (0), 145.55 sec >> obj loss: 1.2057, task at 40: 0.3960\n",
      "Epoch 27 (0), 145.64 sec >> obj loss: 1.1070, task at 40: 0.4784\n",
      "Epoch 28 (0), 146.03 sec >> obj loss: 1.0336, task at 40: 0.5436\n",
      "Epoch 29 (1), 145.81 sec >> obj loss: 1.0670, task at 40: 0.5236\n",
      "Epoch 30 (0), 145.85 sec >> obj loss: 0.9784, task at 40: 0.6087\n",
      "Epoch 31 (0), 145.72 sec >> obj loss: 0.9563, task at 40: 0.6321\n",
      "Epoch 32 (1), 145.89 sec >> obj loss: 0.9699, task at 40: 0.6424\n",
      "Epoch 33 (0), 146.02 sec >> obj loss: 0.9455, task at 40: 0.6803\n",
      "Epoch 34 (0), 145.78 sec >> obj loss: 0.9421, task at 40: 0.6862\n",
      "Epoch 35 (0), 145.99 sec >> obj loss: 0.9268, task at 40: 0.6939\n",
      "Epoch 36 (0), 145.86 sec >> obj loss: 0.9163, task at 40: 0.7286\n",
      "Epoch 37 (0), 145.7 sec >> obj loss: 0.9034, task at 40: 0.7301\n",
      "Epoch 38 (1), 145.97 sec >> obj loss: 0.9377, task at 40: 0.7302\n",
      "Epoch 39 (0), 145.81 sec >> obj loss: 0.8952, task at 40: 0.7529\n",
      "Epoch 40 (1), 146.16 sec >> obj loss: 0.9247, task at 40: 0.7563\n",
      "Epoch 41 (2), 145.93 sec >> obj loss: 0.9081, task at 40: 0.7644\n",
      "Epoch 42 (3), 145.83 sec >> obj loss: 0.9334, task at 40: 0.7596\n",
      "Epoch 43 (4), 145.95 sec >> obj loss: 0.9744, task at 40: 0.7592\n",
      "Epoch 44 (0), 179.72 sec >> obj loss: 2.1222, task at 10: 0.0992\n",
      "Epoch 45 (0), 179.02 sec >> obj loss: 1.9382, task at 10: 0.0946\n",
      "Epoch 46 (0), 179.1 sec >> obj loss: 1.7524, task at 10: 0.1052\n",
      "Epoch 47 (0), 179.07 sec >> obj loss: 1.6464, task at 10: 0.1003\n",
      "Epoch 48 (0), 178.99 sec >> obj loss: 1.6025, task at 10: 0.1027\n",
      "Epoch 49 (0), 178.97 sec >> obj loss: 1.5964, task at 10: 0.1028\n",
      "Epoch 50 (0), 179.1 sec >> obj loss: 1.5909, task at 10: 0.0991\n",
      "Epoch 51 (1), 179.12 sec >> obj loss: 1.5933, task at 10: 0.0992\n",
      "Epoch 52 (0), 179.12 sec >> obj loss: 1.5806, task at 10: 0.0987\n",
      "Epoch 53 (1), 179.4 sec >> obj loss: 1.5958, task at 10: 0.0975\n",
      "Epoch 54 (2), 178.98 sec >> obj loss: 1.5940, task at 10: 0.1002\n",
      "Epoch 55 (3), 178.93 sec >> obj loss: 1.6008, task at 10: 0.0995\n",
      "Epoch 56 (4), 179.01 sec >> obj loss: 1.6068, task at 10: 0.0995\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "tasks = ['100', '70', '40', '10']\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-4\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 4\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # task-wise loss aggregation\n",
    "        obj = tf.add(tf.multiply(obj, 1.0-alpha), tf.multiply(new_obj, alpha))\n",
    "        \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.max\n",
    "            val_accu = val_accu/pval.max\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "\n",
    "#             epoch_summary = sess.run([vgg16.summary_op],\n",
    "#                                     feed_dict={vgg16.x: Xtest,\n",
    "#                                                vgg16.y: Ytest})\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2acd8609bb4435d946c9ce47bae989e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673e745c659f4cf6947a2342fdb2aa5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 61.66 sec >> obj loss: 0.8023, task at 100: 0.7296\n",
      "Epoch 2 (0), 58.42 sec >> obj loss: 0.6412, task at 100: 0.7827\n",
      "Epoch 3 (0), 58.84 sec >> obj loss: 0.5972, task at 100: 0.7976\n",
      "Epoch 4 (0), 58.53 sec >> obj loss: 0.5646, task at 100: 0.8095\n",
      "Epoch 5 (0), 58.69 sec >> obj loss: 0.5438, task at 100: 0.8174\n",
      "Epoch 6 (0), 58.67 sec >> obj loss: 0.5117, task at 100: 0.8307\n",
      "Epoch 7 (1), 55.9 sec >> obj loss: 0.5340, task at 100: 0.8281\n",
      "Epoch 8 (2), 55.53 sec >> obj loss: 0.5481, task at 100: 0.8298\n",
      "Epoch 9 (0), 58.92 sec >> obj loss: 0.4975, task at 100: 0.8409\n",
      "Epoch 10 (1), 55.66 sec >> obj loss: 0.5036, task at 100: 0.8461\n",
      "Epoch 11 (2), 55.6 sec >> obj loss: 0.5543, task at 100: 0.8455\n",
      "Epoch 12 (3), 55.61 sec >> obj loss: 0.5484, task at 100: 0.8459\n",
      "Epoch 13 (0), 93.82 sec >> obj loss: 0.6262, task at 80: 0.7637\n",
      "Epoch 14 (0), 92.99 sec >> obj loss: 0.5885, task at 80: 0.8066\n",
      "Epoch 15 (1), 90.01 sec >> obj loss: 0.6169, task at 80: 0.8116\n",
      "Epoch 16 (2), 90.02 sec >> obj loss: 0.5947, task at 80: 0.8196\n",
      "Epoch 17 (3), 89.7 sec >> obj loss: 0.6319, task at 80: 0.8209\n",
      "Epoch 18 (0), 127.55 sec >> obj loss: 0.6717, task at 70: 0.7754\n",
      "Epoch 19 (0), 126.56 sec >> obj loss: 0.6504, task at 70: 0.7961\n",
      "Epoch 20 (0), 127.08 sec >> obj loss: 0.6459, task at 70: 0.8106\n",
      "Epoch 21 (0), 126.9 sec >> obj loss: 0.6307, task at 70: 0.8224\n",
      "Epoch 22 (1), 123.52 sec >> obj loss: 0.6423, task at 70: 0.8255\n",
      "Epoch 23 (2), 123.13 sec >> obj loss: 0.6794, task at 70: 0.8171\n",
      "Epoch 24 (3), 123.2 sec >> obj loss: 0.7544, task at 70: 0.8050\n",
      "Epoch 25 (0), 161.22 sec >> obj loss: 0.6801, task at 60: 0.7829\n",
      "Epoch 26 (1), 157.05 sec >> obj loss: 0.6877, task at 60: 0.7952\n",
      "Epoch 27 (0), 160.8 sec >> obj loss: 0.6709, task at 60: 0.8069\n",
      "Epoch 28 (1), 156.75 sec >> obj loss: 0.7496, task at 60: 0.7951\n",
      "Epoch 29 (2), 156.91 sec >> obj loss: 0.7397, task at 60: 0.8030\n",
      "Epoch 30 (3), 156.93 sec >> obj loss: 0.7416, task at 60: 0.8141\n",
      "Epoch 31 (0), 194.81 sec >> obj loss: 0.7257, task at 55: 0.7886\n",
      "Epoch 32 (0), 194.02 sec >> obj loss: 0.6949, task at 55: 0.8024\n",
      "Epoch 33 (1), 190.05 sec >> obj loss: 0.7216, task at 55: 0.8008\n",
      "Epoch 34 (2), 190.31 sec >> obj loss: 0.7413, task at 55: 0.8090\n",
      "Epoch 35 (3), 190.56 sec >> obj loss: 0.7409, task at 55: 0.8207\n",
      "Epoch 36 (0), 227.57 sec >> obj loss: 0.6922, task at 50: 0.7933\n",
      "Epoch 37 (0), 227.27 sec >> obj loss: 0.6822, task at 50: 0.8114\n",
      "Epoch 38 (1), 223.37 sec >> obj loss: 0.6895, task at 50: 0.8150\n",
      "Epoch 39 (2), 223.63 sec >> obj loss: 0.7094, task at 50: 0.8139\n",
      "Epoch 40 (3), 223.67 sec >> obj loss: 0.7432, task at 50: 0.8213\n",
      "Epoch 41 (0), 261.79 sec >> obj loss: 0.7354, task at 45: 0.7743\n",
      "Epoch 42 (1), 256.05 sec >> obj loss: 0.7465, task at 45: 0.7812\n",
      "Epoch 43 (2), 256.6 sec >> obj loss: 0.7575, task at 45: 0.7913\n",
      "Epoch 44 (3), 256.32 sec >> obj loss: 0.7593, task at 45: 0.8002\n",
      "Epoch 45 (0), 294.86 sec >> obj loss: 0.7586, task at 40: 0.7556\n",
      "Epoch 46 (0), 294.75 sec >> obj loss: 0.7203, task at 40: 0.7857\n",
      "Epoch 47 (0), 294.35 sec >> obj loss: 0.7166, task at 40: 0.7983\n",
      "Epoch 48 (1), 289.82 sec >> obj loss: 0.7854, task at 40: 0.7891\n",
      "Epoch 49 (2), 289.8 sec >> obj loss: 0.8274, task at 40: 0.7881\n",
      "Epoch 50 (3), 289.43 sec >> obj loss: 0.8632, task at 40: 0.7716\n",
      "Epoch 51 (0), 328.53 sec >> obj loss: 0.7993, task at 35: 0.7510\n",
      "Epoch 52 (0), 327.14 sec >> obj loss: 0.7681, task at 35: 0.7779\n",
      "Epoch 53 (1), 322.31 sec >> obj loss: 0.7703, task at 35: 0.7876\n",
      "Epoch 54 (2), 322.74 sec >> obj loss: 0.7728, task at 35: 0.7961\n",
      "Epoch 55 (3), 322.2 sec >> obj loss: 0.8166, task at 35: 0.7890\n",
      "Epoch 56 (0), 364.15 sec >> obj loss: 0.8919, task at 30: 0.6828\n",
      "Epoch 57 (1), 358.05 sec >> obj loss: 0.9330, task at 30: 0.6950\n",
      "Epoch 58 (0), 363.46 sec >> obj loss: 0.7770, task at 30: 0.7566\n",
      "Epoch 59 (1), 362.04 sec >> obj loss: 0.7981, task at 30: 0.7577\n",
      "Epoch 60 (2), 362.37 sec >> obj loss: 0.7893, task at 30: 0.7710\n",
      "Epoch 61 (3), 363.03 sec >> obj loss: 0.7962, task at 30: 0.7832\n",
      "Epoch 62 (0), 399.29 sec >> obj loss: 0.8565, task at 25: 0.6943\n",
      "Epoch 63 (0), 397.68 sec >> obj loss: 0.7843, task at 25: 0.7433\n",
      "Epoch 64 (1), 393.35 sec >> obj loss: 0.8251, task at 25: 0.7455\n",
      "Epoch 65 (0), 399.19 sec >> obj loss: 0.7775, task at 25: 0.7674\n",
      "Epoch 66 (1), 393.92 sec >> obj loss: 0.8027, task at 25: 0.7682\n",
      "Epoch 67 (2), 392.16 sec >> obj loss: 0.8572, task at 25: 0.7672\n",
      "Epoch 68 (3), 392.03 sec >> obj loss: 0.8345, task at 25: 0.7761\n",
      "Epoch 69 (0), 434.1 sec >> obj loss: 0.9828, task at 20: 0.6269\n",
      "Epoch 70 (0), 432.62 sec >> obj loss: 0.9283, task at 20: 0.6858\n",
      "Epoch 71 (0), 432.46 sec >> obj loss: 0.8691, task at 20: 0.7054\n",
      "Epoch 72 (0), 432.48 sec >> obj loss: 0.8427, task at 20: 0.7320\n",
      "Epoch 73 (0), 431.21 sec >> obj loss: 0.8190, task at 20: 0.7490\n",
      "Epoch 74 (1), 425.35 sec >> obj loss: 0.8382, task at 20: 0.7540\n",
      "Epoch 75 (2), 423.86 sec >> obj loss: 0.8570, task at 20: 0.7540\n",
      "Epoch 76 (3), 425.04 sec >> obj loss: 0.9257, task at 20: 0.7340\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_path = os.path.join('save/', 'model2.ckpt')\n",
    "\n",
    "tasks = ['100', '80', '70', '60', '55', '50', '45', '40', '35' ,'30', '25', '20']\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-4\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 3\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # task-wise loss aggregation\n",
    "        obj = tf.add(tf.multiply(obj, 1.0-alpha), tf.multiply(new_obj, alpha))\n",
    "        \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/200)):\n",
    "                st = i*200\n",
    "                ed = (i+1)*200\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.value\n",
    "            val_accu = val_accu/pval.value\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "\n",
    "#             epoch_summary = sess.run([vgg16.summary_op],\n",
    "#                                     feed_dict={vgg16.x: Xtest,\n",
    "#                                                vgg16.y: Ytest})\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881f15d2b50f4bd4a1f0367375688996"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c8e03c89a046e088ca32e06a519655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 61.75 sec >> obj loss: 0.8049, task at 100: 0.7365\n",
      "Epoch 2 (0), 65.58 sec >> obj loss: 0.6629, task at 100: 0.7823\n",
      "Epoch 3 (0), 66.29 sec >> obj loss: 0.6414, task at 100: 0.7927\n",
      "Epoch 4 (0), 66.03 sec >> obj loss: 0.5521, task at 100: 0.8281\n",
      "Epoch 5 (0), 66.16 sec >> obj loss: 0.5505, task at 100: 0.8329\n",
      "Epoch 6 (1), 56.43 sec >> obj loss: 0.5676, task at 100: 0.8293\n",
      "Epoch 7 (0), 58.77 sec >> obj loss: 0.5269, task at 100: 0.8482\n",
      "Epoch 8 (1), 55.39 sec >> obj loss: 0.5635, task at 100: 0.8440\n",
      "Epoch 9 (2), 55.46 sec >> obj loss: 0.5996, task at 100: 0.8380\n",
      "Epoch 10 (3), 55.55 sec >> obj loss: 0.6469, task at 100: 0.8394\n",
      "Epoch 11 (0), 94.0 sec >> obj loss: 0.6175, task at 80: 0.8443\n",
      "Epoch 12 (1), 91.17 sec >> obj loss: 0.6550, task at 80: 0.8472\n",
      "Epoch 13 (2), 91.09 sec >> obj loss: 0.6739, task at 80: 0.8421\n",
      "Epoch 14 (3), 90.09 sec >> obj loss: 0.6579, task at 80: 0.8525\n",
      "Epoch 15 (0), 127.16 sec >> obj loss: 0.6478, task at 70: 0.8401\n",
      "Epoch 16 (1), 123.04 sec >> obj loss: 0.7368, task at 70: 0.8385\n",
      "Epoch 17 (2), 123.24 sec >> obj loss: 0.7395, task at 70: 0.8357\n",
      "Epoch 18 (3), 123.04 sec >> obj loss: 0.7919, task at 70: 0.8386\n",
      "Epoch 19 (0), 161.54 sec >> obj loss: 0.8000, task at 60: 0.8153\n",
      "Epoch 20 (0), 161.22 sec >> obj loss: 0.7666, task at 60: 0.8349\n",
      "Epoch 21 (1), 157.4 sec >> obj loss: 0.8685, task at 60: 0.8333\n",
      "Epoch 22 (2), 157.75 sec >> obj loss: 0.8428, task at 60: 0.8386\n",
      "Epoch 23 (3), 158.01 sec >> obj loss: 0.8215, task at 60: 0.8402\n",
      "Epoch 24 (0), 195.38 sec >> obj loss: 0.7790, task at 55: 0.8339\n",
      "Epoch 25 (1), 190.65 sec >> obj loss: 0.8358, task at 55: 0.8297\n",
      "Epoch 26 (2), 191.11 sec >> obj loss: 0.7796, task at 55: 0.8368\n",
      "Epoch 27 (3), 190.39 sec >> obj loss: 0.8369, task at 55: 0.8378\n",
      "Epoch 28 (0), 231.85 sec >> obj loss: 0.7633, task at 50: 0.8243\n",
      "Epoch 29 (1), 226.78 sec >> obj loss: 0.8162, task at 50: 0.8266\n",
      "Epoch 30 (2), 227.26 sec >> obj loss: 0.8897, task at 50: 0.8282\n",
      "Epoch 31 (3), 227.37 sec >> obj loss: 0.8415, task at 50: 0.8262\n",
      "Epoch 32 (0), 266.17 sec >> obj loss: 0.7379, task at 45: 0.8291\n",
      "Epoch 33 (1), 259.72 sec >> obj loss: 0.8159, task at 45: 0.8166\n",
      "Epoch 34 (2), 260.38 sec >> obj loss: 0.9044, task at 45: 0.8136\n",
      "Epoch 35 (3), 261.53 sec >> obj loss: 0.8232, task at 45: 0.8313\n",
      "Epoch 36 (0), 298.92 sec >> obj loss: 0.8402, task at 40: 0.8000\n",
      "Epoch 37 (1), 293.84 sec >> obj loss: 0.8462, task at 40: 0.8074\n",
      "Epoch 38 (2), 293.33 sec >> obj loss: 0.8566, task at 40: 0.8208\n",
      "Epoch 39 (3), 293.48 sec >> obj loss: 0.9001, task at 40: 0.8221\n",
      "Epoch 40 (0), 333.23 sec >> obj loss: 0.8902, task at 35: 0.7857\n",
      "Epoch 41 (1), 326.55 sec >> obj loss: 0.9081, task at 35: 0.8041\n",
      "Epoch 42 (2), 325.41 sec >> obj loss: 0.9479, task at 35: 0.8028\n",
      "Epoch 43 (3), 325.31 sec >> obj loss: 0.9348, task at 35: 0.8120\n",
      "Epoch 44 (0), 365.85 sec >> obj loss: 0.8330, task at 30: 0.7784\n",
      "Epoch 45 (1), 359.27 sec >> obj loss: 0.8329, task at 30: 0.8056\n",
      "Epoch 46 (2), 359.18 sec >> obj loss: 0.9011, task at 30: 0.7914\n",
      "Epoch 47 (3), 362.14 sec >> obj loss: 0.9665, task at 30: 0.7834\n",
      "Epoch 48 (0), 398.99 sec >> obj loss: 0.8383, task at 25: 0.7709\n",
      "Epoch 49 (1), 391.44 sec >> obj loss: 0.8841, task at 25: 0.7763\n",
      "Epoch 50 (2), 391.49 sec >> obj loss: 0.9228, task at 25: 0.7740\n",
      "Epoch 51 (3), 390.6 sec >> obj loss: 0.9984, task at 25: 0.7746\n",
      "Epoch 52 (0), 431.06 sec >> obj loss: 0.9134, task at 20: 0.7520\n",
      "Epoch 53 (0), 431.27 sec >> obj loss: 0.8344, task at 20: 0.7621\n",
      "Epoch 54 (1), 423.18 sec >> obj loss: 0.8964, task at 20: 0.7593\n",
      "Epoch 55 (2), 425.31 sec >> obj loss: 0.9118, task at 20: 0.7723\n",
      "Epoch 56 (3), 425.84 sec >> obj loss: 0.9826, task at 20: 0.7708\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_path = os.path.join('save_linear/', 'model.ckpt')\n",
    "\n",
    "tasks = ['100', '80', '70', '60', '55', '50', '45', '40', '35' ,'30', '25', '20']\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-3\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 3\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # task-wise loss aggregation\n",
    "        obj = tf.add(tf.multiply(obj, 1.0-alpha), tf.multiply(new_obj, alpha))\n",
    "        \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/200)):\n",
    "                st = i*200\n",
    "                ed = (i+1)*200\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.value\n",
    "            val_accu = val_accu/pval.value\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "\n",
    "#             epoch_summary = sess.run([vgg16.summary_op],\n",
    "#                                     feed_dict={vgg16.x: Xtest,\n",
    "#                                                vgg16.y: Ytest})\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# not alpha balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9ba49baf804132b0a55eb6ac23154e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be933afdbed94d0fad2e1218c2026ae1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0), 55.91 sec >> obj loss: 2.5209, task at 100: 0.1012\n",
      "Epoch 2 (0), 54.99 sec >> obj loss: 2.4462, task at 100: 0.1018\n",
      "Epoch 3 (1), 55.07 sec >> obj loss: 2.4867, task at 100: 0.0994\n",
      "Epoch 4 (0), 55.29 sec >> obj loss: 2.3946, task at 100: 0.1003\n",
      "Epoch 5 (1), 55.12 sec >> obj loss: 2.4218, task at 100: 0.1008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7bcfb3dc8ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0med\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n\u001b[0;32m---> 76\u001b[0;31m                                                 vgg16.y: Ytrain[st:ed,:]})\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mptrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mptrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training %s/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tasks = ['100', '80', '70', '60', '55', '50', '45', '40', '35' ,'30', '25', '20']\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=len(tasks))\n",
    "checkpoint_path = os.path.join('/data/put_data/cmchang/save_linear_no_alpha/', 'model.ckpt')\n",
    "\n",
    "# initial task\n",
    "obj = vgg16.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # hyper parameters\n",
    "    learning_rate = 2e-3\n",
    "    batch_size = 32\n",
    "    alpha = 0.5\n",
    "    early_stop_patience = 4\n",
    "    min_delta = 0.0001\n",
    "    \n",
    "    # optimizer\n",
    "    #opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer = tf.summary.FileWriter(\"log_no_alpha/\", sess.graph)\n",
    "    \n",
    "    # progress bar\n",
    "    from progress.bar import Bar\n",
    "    \n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xtest.shape[0]/batch_size)\n",
    "    \n",
    "    while(len(tasks)):\n",
    "       \n",
    "        # acquire a new task\n",
    "        cur_task = tasks[0]\n",
    "        tasks = tasks[1:]\n",
    "        new_obj = vgg16.loss_dict[cur_task]\n",
    "        \n",
    "        # just finished a task\n",
    "        if epoch_counter > 0:\n",
    "            # save models\n",
    "            saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "            \n",
    "            # task-wise loss aggregation\n",
    "            obj = tf.add(obj, new_obj)\n",
    "            \n",
    "        # optimizer\n",
    "        train_op = opt.minimize(obj)\n",
    "        \n",
    "        # re-initialize\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "        # optimize when the aggregated obj\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            # training an epoch\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                sess.run([train_op], feed_dict={vgg16.x: Xtrain[st:ed,:,:,:],\n",
    "                                                vgg16.y: Ytrain[st:ed,:]})\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            # validation\n",
    "            val_loss = 0\n",
    "            val_accu = 0\n",
    "            for i in range(int(Xtest.shape[0]/200)):\n",
    "                st = i*200\n",
    "                ed = (i+1)*200\n",
    "                loss, accu, epoch_summary = sess.run([obj, vgg16.accu_dict[cur_task], vgg16.summary_op],\n",
    "                                    feed_dict={vgg16.x: Xtest[st:ed,:],\n",
    "                                               vgg16.y: Ytest[st:ed,:]})\n",
    "                val_loss += loss\n",
    "                val_accu += accu\n",
    "                pval.value += 1\n",
    "                pval.description = \"Testing %s/%s\" % (i, pval.max)\n",
    "            #val_loss = np.mean(val_loss,axis=-1)\n",
    "            #val_accu = np.mean(val_accu,axis=-1)\n",
    "            val_loss = val_loss/pval.value\n",
    "            val_accu = val_accu/pval.value\n",
    "            \n",
    "            # early stopping check\n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            idx = np.random.permutation(Xtrain.shape[0])\n",
    "            Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "            \n",
    "            # epoch end\n",
    "            writer.add_summary(epoch_summary, epoch_counter)\n",
    "            epoch_counter += 1\n",
    "            \n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            \n",
    "            print(\"Epoch %s (%s), %s sec >> obj loss: %.4f, task at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), val_loss, cur_task, val_accu))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "accu_log = []\n",
    "loss_log = []\n",
    "tag_log = []\n",
    "counter = 0\n",
    "for e in tf.train.summary_iterator(\"log/events.out.tfevents.1516590873.theta1\"):\n",
    "    counter += 1\n",
    "    for v in e.summary.value:\n",
    "        if 'accu' in v.tag:\n",
    "            tag_log.append(str(counter-3)+\"/\"+v.tag)\n",
    "            accu_log.append(v.simple_value)\n",
    "        if 'loss' in v.tag:\n",
    "            loss_log.append(v.simple_value)\n",
    "\n",
    "output = pd.DataFrame.from_dict({'tag':tag_log, 'loss':loss_log, 'accu':accu_log})\n",
    "output.to_csv(\"output.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
